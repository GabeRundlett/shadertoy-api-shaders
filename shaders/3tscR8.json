{
    "Shader": {
        "info": {
            "date": "1591792254",
            "description": "Low sample realtime 2D global illumination of a bidirectional Truchet field with temporal camera reprojection to give the impression of higher sampled imagery.",
            "flags": 32,
            "hasliked": 0,
            "id": "3tscR8",
            "likes": 73,
            "name": "2D Realtime Path Tracing",
            "published": 3,
            "tags": [
                "global",
                "illumination",
                "truchet",
                "tracing",
                "path",
                "denoise",
                "reprojection",
                "radiosity"
            ],
            "usePreview": 0,
            "username": "Shane",
            "viewed": 2480
        },
        "renderpass": [
            {
                "code": "/*\n\n\t2D Realtime Path Tracing\n\t------------------------\n\n\t2D path tracing, or just one ultra expensive Voronoi-Truchet texture, depending on \n    your perspective. :) Technically speaking, this is a very low-sample path traced \n    rendering of a 2D distance field that has been stored in the channels of a screen \n    buffer, then camera reprojected a few times to give the impression of a much higher \n    sampling... I'll explain that in more detail further down the page. :) The 2D \n    distance field itself is an animated bidirectional Truchet pattern, encoded into \n    one of the faces of the cube map.\n\n\tI love the 2D path tracing aesthetic. In fact, I even like the low sampled realtime\n    noisy imagery as well, but that's a personal thing. Not everyone likes digital noise, \n    and not everyone likes the expense of path tracing, which is fair enough, so that's \n    why you don't see too many examples on here. There's also the issue of being forced \n    to construct uninspiring low instruction scenery, which can be limiting as well.\n\n    However, it's possible to use a buffer with precalculated values to enable better \n    scenery, which I'm sure most are aware of. What people may not know, however, is\n    that there are great denoising techniques to alleviate noise issues -- Not entirely, \n    but enough so that some people might like to give realtime path tracing, or path \n    tracing in general, a try.\n\n    In this case, each screen render is stored in a buffer and mixed with the next to \n    blend in the noise. It's very effective on static imagery for anyone who's tried it. \n    Unfortunately, with moving imagery, you see ghosting trails, which look kind of cool, \n    but don't help with the illusion. Thankfully, it's possible to calculate  where the \n    current screen render is in relation to the previous one, then blend in the previous \n    render at the new position. This alleviates ghosting to quite a large degree. The end \n\tresult is a low sampled moving image that looks like (or is, in fact) a high sampled, \n    much more appealing one... to most people. Ironically, I love the noisy, pixelated \n    fake 90s demo aesthetic, which means this is redundant to me. :D\n\n\tAnyway, this is just a proof of concept to show that it's possible, but I wouldn't \n    pay too much attention to the code itself... It's correct enough to get the job done, \n    but was slapped together pretty quickly. Honestly, I wouldn't want to try to\n    interpret this, but I've commented it to enable anyone to get the general idea.\n    \n    Additionally, there are a few compiler directives in the \"Common\" tab to try that \n    might help.\n\n    By the way, this can be extended to 3D situations as well, and I intend to show that \n\tat some stage. In the meantime, IQ has an awesome 3D gloabally illuminated example, \n\tcomplete with denoising camera reprojection on Shadertoy that's well worth the look, \n    especially since examples like that are thin on the ground. On a side note, I remember \n    reading somewhere that if it wasn't for IQ, the average graphics programmer would be \n    several years behind where they are today. :)\n\n\n    Useful links:\n\n\t// 3D temporal reprojection: IQ puts up a lot of difficult to find code with\n    // very little fanfare. This is one example.\n    Some boxes - iq\n    https://www.shadertoy.com/view/Xd2fzR\n\n\n*/\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord){\n\n    // Retrieving the stored color.\n    vec4 col = texture(iChannel0, fragCoord/iResolution.xy);\n\n    // Gamma correction and screen presentation.\n    fragColor = pow(col, vec4(.4545));\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "// Temporal camera reprojection: Without this option, you can see what the original\n// looks like without the higher sampling this technique brings. Just for the record,\n// the noisy image has a certain kind of shabby chic appeal to me. :D\n#define TEMPORAL_REPROJECTION\n\n// Show the Truchet pattern, or not. The alternative is just a grid of circles.\n// If you change this from the Truchet pattern to the circles one, remember to hit the \n// back\\reset button to reload the cube map.\n//\n// The Truchet pattern is more interesting, but I believe the static circles play off\n// the light better.\n#define TRUCHET_PATTERN\n\n\n// Even cheaper without the light tracing, but the shadows won't be there. \n// However, I've darkened the surrounds to give that effect.\n#define LIGHT_TRACE\n\n// Display the underlying distance field. Sometimes, this can be helpful. The\n// lighting definitely isn't as interesting though.\n//#define DISTANCE_FIELD_ONLY\n\n\n// Grid boundaries.\n//#define SHOW_GRID\n\n\n// Display the glowing light. It just a glowing blob moving in front of the\n// camera through the pattern.\n//#define SHOW_LIGHT\n\n\n// Distance field object ID. For the Truchet case, we're simply flagging the\n// inner and outer rails to give them some color.\nvec2 gIP;\n\n// Grid pattern repeat scale. Baking wrapped distance fields into textures can be \n// a little fiddly. Basically, the pattern is wrapped on a 32 by 32 unit basis.\nfloat repSc = 32.;\n\n// The cubemap texture resolution.\n//#define iRes0 vec2(1024)\n\n// Standard 2D rotation formula.\nmat2 rot2(in float a){ float c = cos(a), s = sin(a); return mat2(c, -s, s, c); }\n\n\n\n// IQ's vec2 to float hash.\nfloat hash21(vec2 p){  return fract(sin(dot(p, vec2(27.619, 57.583)))*43758.5453); }\n\nvec2 hash22(vec2 p) {\n    //return vec2(0);\n    return fract(sin(vec2(dot(p, vec2(12.989, 78.233)), dot(p, vec2(41.898, 57.263))))\n                      *vec2(43758.5453, 23421.6361));\n}\n\n// IQ's vec2 to float hash, but with a repeat factor. If you repeat random\n// textures to wrap, then you need to wrap the random functions.\nfloat hash21Rep(vec2 p){ \n    p = mod(p, repSc); \n    return fract(sin(dot(p, vec2(27.619, 57.583)))*43758.5453); \n}\n\nvec2 hash22Rep(vec2 p) {\n    //return vec2(0);\n    p = mod(p, repSc);\n    return fract(sin(vec2(dot(p, vec2(12.989, 78.233)), dot(p, vec2(41.898, 57.263))))\n                      *vec2(43758.5453, 23421.6361));\n}\n\n// Believe it or not, the simple one-line function below took me ages to figure out. The \n// only refrences to it seem to be from some Microsoft documentation somewhere, because it's \n// all written in some obscure way that involves the term, \"fract(p)*2. - 1.,\" etc.\n//\n// Anyway, the following should have been obvious to me, but it wasn't: A unit cube centered \n// on a grid has six faces with a center at vec3(0), and 8 vertices at coordinates, \n// vec3(-.5, -.5, -.5), vec3(-.5, -.5, .5), etc. Therefore, using very basic UV mapping logic, \n// the faces will be the following:\n//\n// Left face: \n// // Wrapping and centering coordinates on the YZ plain: \n// p.yz = fract(p.yz) - .5;\n// // The X coordinate is at \"-.5\".\n// p.x = -.5;\n// // Texture coordinate. \n// vec4 tx = texture(texChannel, vec3(-.5, fract(p.yz) - .5));\n//\n// All faces follow the same logic, with a bit of UV flipping to get things facing the right \n// way, etc. Using uv = fract(uv) - .5:\n//\n// The four cube sides - Left, back, right, front.\n// NEGATIVE_X, POSITIVE_Z, POSITIVE_X, NEGATIVE_Z\n// vec3(-.5, uv.yx), vec3(uv, .5), vec3(.5, uv.y, -uv.x), vec3(-uv.x, uv.y, -.5).\n//\n// Bottom and top.\n// NEGATIVE_Y, POSITIVE_Y\n// vec3(uv.x, -.5, uv.y), vec3(uv.x, .5, -uv.y).\n//\n//\n// From what I've noticed, the size of the cube you use doesn't seem to matter -- I'm \n// assuming this is due to an internal normalization process. Therefore, to save extra \n// calculations (which matter when doing 3D stuff), you may as well use the unit cube \n// figures above -- instead of vec3(fract(p)*2. - 1., 1), vec3(fract(p) - .5)*n, n), etc.\n \n\n// Reading in the texture from the right face of the cube: I chose this because it \n// writes more easily, but you can read from any, or as many, faces you'd like. I'm\n// assuming that all sides index into memory at the same rate, otherwise you'd have to\n// take that into consideration when favoring one side or the other.\n//\n// By the way, \"p\" is simply your \"uv\" coordinates, which are usually: \n// uv = fragCoord/iResolution.y, but could represent cube sides, like p.xz, etc.\nvec4 tx(samplerCube tx, vec2 p){    \n\n    return texture(tx, vec3(fract(p) - .5, .5));\n}\n\n// IQ's box function, with modified smoothing element.\nfloat sBoxS(in vec2 p, in vec2 b, in float rf){\n  \n  vec2 d = abs(p) - b + rf;\n  return min(max(d.x, d.y), 0.) + length(max(d, 0.)) - rf;\n    \n}\n\n// IQ's box function.\nfloat sBox(in vec2 p, in vec2 b){\n    \n  vec2 d = abs(p) - b;\n  return min(max(d.x, d.y), 0.) + length(max(d, 0.));\n}\n",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            },
            {
                "code": "float dfGridObjects(vec2 p, vec2 ip, float sc){\n    \n    ip = mod(ip, repSc);\n  \n    float rnd = hash21(ip);\n    vec2 rnd2 = hash22(ip);\n    \n    if(ip.x < .001) {\n        sc *= .7; \n        \n    }\n    \n    float sz = (.07 + rnd*.15)*sc*1.55;\n    float d  = length(p - (rnd2 - .5)*.5*sc*1.) - sz;\n    // Rotated Manhattan... Needs work, so circles it is. :)\n    //p *= rot2(rnd*3.14159/4.);\n    //float d = sBoxS(p - (rnd2 - .5)*.5*sc*1.2, vec2(sz*1.3), .8*sc);\n    \n    \n    gIP = rnd2;\n    \n    return d;\n}\n\n\n// The Truchet pattern -- This one is animated, plus it has inner and outer \n// rails travelling in opposite directions, which I don't recall seeing here,\n// but someone may have done it already.\nfloat dfTruchet(vec2 p, vec2 ip, float sc){\n    \n   \n   \n    ip = mod(ip, repSc);\n    \n    // Unique random cell number.\n    float rnd = hash21(ip);\n    \n    // Horizontally flip random cell tiles.\n    if (rnd < .5) p = p.yx*vec2(1, -1);//p.y = -p.y; // p.x = -p.x, \n    \n    \n    // TRUCHET TILE.\n    //\n    // Two arcs, centered on diagonally opposite grid cell corners.\n    \n    // Circles, in opposite corners.\n    float d1 = length(p - .5*sc) - .5*sc;\n    float d2 = length(p + .5*sc) - .5*sc;\n    \n    \n    // Individual polar coordinates.\n    vec2 uv1 = vec2(d1, atan(p.y - .5*sc, p.x - .5*sc));\n    vec2 uv2 = vec2(d2, atan(p.y + .5*sc, p.x + .5*sc));\n\n   \n     \n    // Switch directions on alternate checkered cells. That's the standard\n    // way it's done.\n    if (mod(ip.x + ip.y, 2.)<.5) {\n        uv1 *= -1.;\n        uv2 *= -1.;\n    }\n    \n    if (rnd < .5){\n        uv1 *= -1.;\n        uv2 *= -1.;\n    }   \n   \n    // UV coordinates for each arc.\n    float gTm = 0.;// Global time: iTime, etc.\n    uv1 = vec2(uv1.x, fract(uv1.y*4./6.2831 + gTm));   \n    uv2 = vec2(uv2.x, fract(uv2.y*4./6.2831 + gTm));   \n\n \n    // Arc thickness.\n    float th = .175/2.*sc;\n\n    \n    // Number of polar partitions. Any more than 2, and it starts to look\n    // too busy.\n    const float aSc = 1.;\n    \n    // Arc length. Range: [0, 1]. \n    float arcL = .6;\n    \n    // Scaling to get to the right range... Circle circumferance, etc. Annoying, fiddly stuff. :)\n    arcL *= sc/3.1459/aSc;\n    \n    // The angular time component, which is set to twice that of the camera time to\n    // allow the light to pass through the open gaps unhindered.\n    float tm = iTime/3.;\n     \n\n    // Using the texture coordinates to render some repeat squares. You do this in the same\n    // way that you'd render any repeat squares. The added complication is the inside and \n    // outside tracks moving in opposing directions, but it's not that difficult.\n    \n    // This relates to the arc tangent (uv.y) normalization process, since we divided by\n    // this to convert so need to compensate. See above.\n    sc *= (4./6.2831);\n    \n    // Inner and outer arcs subtended to the top-left grid cell corner.\n    float tracksA = (mod(uv1.y + tm, 1./aSc) - 1./aSc/2.);\n    float tracksB = (mod(uv1.y - tm, 1./aSc) - 1./aSc/2.);\n\n    float tracks1 = sBoxS(vec2((uv1.x - (th + .002)), tracksA*sc), vec2((th - .002), arcL), .05*sc);\n    float tracks2 = sBoxS(vec2((uv1.x + (th - .002)), tracksB*sc), vec2((th - .002), arcL), .05*sc);\n\n    // Inner and outer arcs subtended to the bottom right grid cell corner.\n    tracksA = (mod(uv2.y + tm, 1./aSc) - 1./aSc/2.);\n    tracksB = (mod(uv2.y - tm, 1./aSc) - 1./aSc/2.);\n    \n    float tracks3 = sBoxS(vec2((uv2.x - (th + .002)), tracksA*sc), vec2((th - .002), arcL), .05*sc);\n    float tracks4 = sBoxS(vec2((uv2.x + (th - .002)), tracksB*sc), vec2((th - .002), arcL), .05*sc);\n\n    // Minimum inner and outer tracks.\n    tracks1 = min(tracks1, tracks3);\n    tracks2 = min(tracks2, tracks4);\n    \n    // ID for inner and out tracks. You could put more effort in here, but\n    // this will do.\n    gIP = tracks1<tracks2? vec2(0) : vec2(1);\n    \n    \n    // Return the minimum distance.\n    return min(tracks1, tracks2); \n    //d = max(d, sBox(p, vec2(.5*sc + .001))); // Used for neighbor checks.\n\n    \n}\n\n\n// Iterating through grid neighbors at a particular scale. Fiddly coding, but necessary.\nvec3 dfNeighbors(vec2 q){\n\n    \n    // Scale, ID, and distance field storage.\n    float sc = 1./repSc;\n    float d = 1e5;\n    vec2 id = vec2(0);\n    \n    \n    // It's funny. There are things that I deep down know won't work, but I'll try them\n    // anyway, because I know how annoying doing it properly will be. This is a rendering\n    // of a grid of offset circles. If you wish to offset them so there's cell overlap, you \n    // need to consider neighbors, which is fine.\n    //\n    // However, if you wish to bounce light around, things can be affected by objects that \n    // are several cells away. In this case, about 8 on either side. This means checking\n    // a crazy number of cells -- The kind of numbers that would fry your GPU. Thankfully,\n    // we can do this once at runtime, and store the overall distance field in a texture, or\n    // one of the cube map faces, which is what we're doing here.\n    \n    int iters = min(0, iFrame) + 8;\n    for(int j = -iters; j<=iters; j++){\n        for(int i = -iters; i<=iters; i++){\n\n \n            vec2 ip = floor(q/sc + vec2(i, j)*sc);\n            \n        \tvec2 p = q - (ip + .5)*sc;\n            \n            float dij = dfGridObjects(p, ip, sc);\n          \n            if(dij<d) {\n                d = dij;\n                id = gIP;//ip;\n            }\n \n        }\n    }\n    \n\n    \n    return vec3(d, id);\n    \n}\n\n\n// Loading a scaled distance field. In this case, it's the Truchet pattern. Technically,\n// it might be more correct to check the neighboring cells, but we're trying to save cycles.\nvec3 dfScale(vec2 p){\n\n    \n    // Scale, ID, and distance field storage.\n    float sc = 2./repSc;\n \n    // Cell ID and scale.\n    vec2 ip = floor(p/sc);\n    p -= (ip + .5)*sc;\n\n    // The Truchet distance field.\n    float d = dfTruchet(p, ip, sc);\n    \n    // Return the distance and object ID.\n    return vec3(d, gIP);\n    \n}\n\nvoid mainCubemap(out vec4 fragColor, in vec2 fragCoord, in vec3 rayOri, in vec3 rayDir){\n    \n    \n    // UV coordinates.\n    //\n    // For whatever reason (which I'd love expained), the Y coordinates flip each\n    // frame if I don't negate the coordinates here -- I'm assuming this is internal, \n    // a VFlip thing, or there's something I'm missing. If there are experts out there, \n    // any feedback would be welcome. :)\n    vec2 uv = fract(fragCoord/iResolution.y*vec2(1, -1));\n  \n    // Pixel storage.\n    vec3 col;\n   \n    // Initial conditions -- Performed just the once upon initialization.\n    //if(abs(tx(iChannel0, uv).w - iResolution.y)>.001){\n    //\n    // IQ gave me the following tip, which saved me a heap of trouble and an extra channel. \n    // I'm not sure how he figured this out, but he pretty much knows everything. :D\n    //\n    // If the texture hasn't loaded, or if we're on the first frame, initialize whatever \n    // you wish to initialize. In this case, I'm precalculating an expensive distance\n    // field and storing it in one of the cube map faces.\n    #ifndef TRUCHET_PATTERN\n    if(textureSize(iChannel0, 0).x<2 || iFrame<1){\n        \n        // INITIAL CONDITIONS.\n       \n        // Construct a distance field whilst seting the wrapping value, then store it.\n        repSc = 16.;\n        col = dfNeighbors(uv); // Distance field in X, and object IDs in YZ.\n        //col.yz += .5;\n        \n    }\n    else col = tx(iChannel0, uv).xyz;\n    #else \n    \n    // Construct a distance field whilst seting the wrapping value, then store it.\n    repSc = 16.;\n    col = dfScale(uv); // Distance field in X, and object IDs in YZ.\n     \n    #endif\n\n    // Store in the cube map.\n    fragColor = vec4(col, 1.);\n    \n}\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "cubemap",
                        "id": 41,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/a//media/previz/cubemap00.png"
                    }
                ],
                "name": "Cube A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 41
                    }
                ],
                "type": "cubemap"
            },
            {
                "code": "/*\n\n\tThe 2D path tracing code itself...  I'm not even sure that\n\tyou'd strickly call it path tracing, but it's randomly \n\tsampled rays sent off in random directions that are \n\treflected off of objects according to their surface \n\tproperties, so close enough. Obviously, if you want a \n\ttreatise on the process itself, there are heaps of awesome\n\texamples on here already.\n\n\tOn a side note, I intend to put up a few myself at some \n\tstage.\n\n\n*/\n\n\n\nfloat gridField(vec2 p){\n    \n    vec2 ip = floor(p);\n    p = abs(p - ip - .5);\n\n    return abs(max(p.x, p.y) - .5) - .003;\n}\n\n\n// Reading the distance field from the texture map.\nfloat map(vec2 p){\n\t// Reading distance fields from a texture means taking scaling into\n    // consideration. If you zoom coordinates by a scalar (4, in this case), \n    // you need to scale the return distance value accordingly... Why does \n    // everything have to be so difficult? :D\n    const float sc = 4.;\n    vec4 tex = tx(iChannel0, p/sc);\n    gIP = tex.yz; // The object ID is stored in the YZ channels..\n    return tex.x*sc;\n}\n\n\n\n#define FAR 8.\n#define DELTA .003\n#define RSF 1.\n#define RSF_SHAD 1.\n\nfloat trace(vec2 o, vec2 r){\n    \n    // Raymarching.\n    float d, t = 0.;\n    \n    // 96 iterations here: If speed and complilation time is a concern, choose the smallest \n    // number you can get away with. Apparently, swapping the zero for min(0, frame) can\n    // force the compliler to not unroll the loop, so that can help sometimes too.\n    for(int i=0; i<16; i++){\n        \n        // Surface distance.\n        d = map(o + r*t);\n        \n        // In most cases, the \"abs\" call can reduce artifacts by forcing the ray to\n        // close in on the surface by the set distance from either side. Because this is\n        // two dimensional, it appears to be necessary -- rather than an option -- to avoid \n        // negative values... I haven't thought it through enough, but basically, 2D \n        // raymarching, or whatever you wish to call the process, works differently.\n        //\n        // Equivalent to abs(d)<DELTA... I'll assume it's faster, but I can't be sure.\n        if(d*d<DELTA*DELTA || t>FAR) break;\n        //if(d<DELTA || t>FAR) break;\n        \n        // No ray shortening is needed here, and in an ideal world, you'd never need it, but \n        // sometimes, something like \"t += d*.7\" will be the only easy way to reduce artifacts.\n        t += d*RSF;\n    }\n    \n    t = min(t, FAR); // Clipping to the far distance, which helps avoid artifacts.\n    \n    return t;\n    \n}\n\nfloat lightTrace(vec2 o, vec2 r, float maxDst){\n    \n    // Raymarching.\n    float d, t = 0.;\n    \n    \n    // 96 iterations here: If speed and complilation time is a concern, choose the smallest \n    // number you can get away with. Apparently, swapping the zero for min(0, frame) can\n    // force the compliler to not unroll the loop, so that can help sometimes too.\n    for(int i=0; i<16; i++){\n        \n        // Surface distance.\n        d = map(o + r*t);\n        \n        // In most cases, the \"abs\" call can reduce artifacts by forcing the ray to\n        // close in on the surface by the set distance from either side.\n        if(d<0. || t>maxDst) break;\n        \n        \n        // No ray shortening is needed here, and in an ideal world, you'd never need it, but \n        // sometimes, something like \"t += d*.7\" will be the only easy way to reduce artifacts.\n        t += d*RSF_SHAD;\n    }\n    \n    //t = min(t, maxDst); // Clipping to the far distance, which helps avoid artifacts.\n    \n    return t;\n    \n}\n\n/*\nfloat shadowTrace(vec2 o, vec2 r){\n    \n    // Raymarching.\n    float d, t = 0.;\n    \n    \n    // 96 iterations here: If speed and complilation time is a concern, choose the smallest \n    // number you can get away with. Apparently, swapping the zero for min(0, frame) can\n    // force the compliler to not unroll the loop, so that can help sometimes too.\n    for(int i=0; i<16;i++){\n        \n        // Surface distance.\n        d = map(o + r*t);\n        \n        // In most cases, the \"abs\" call can reduce artifacts by forcing the ray to\n        // close in on the surface by the set distance from either side.\n        if(d<0. || t>FAR) break;\n        \n        \n        // No ray shortening is needed here, and in an ideal world, you'd never need it, but \n        // sometimes, something like \"t += d*.7\" will be the only easy way to reduce artifacts.\n        t += d*RSF_SHAD;\n    }\n    \n    t = min(t, FAR); // Clipping to the far distance, which helps avoid artifacts.\n    \n    return t;\n    \n}\n*/\n\n// Standard 2D normal function.\nvec2 nr(in vec2 p){\n    \n\tconst vec2 e = vec2(.001, 0);\n\treturn normalize(vec2(map(p + e.xy) - map(p - e.xy), \n                          map(p + e.yx) - map(p - e.yx)));\n}\n\n// Translating the camera.\nvec2 getCamTrans(float t){ return vec2(sin(t/8.)/8., -t/6.); }\n\n// Rotating the camera.\nmat2 getCamRot(float t){\n    \n    //return rot2(0.);\n    return rot2(cos(t/4.)/8.);\n}\n\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord){\n\n    // Aspect correct screen coordinates.\n\tvec2 uv = (fragCoord - iResolution.xy*.5)/iResolution.y;\n    \n    \n    \n    // The overall scene color.\n    vec3 col = vec3(0);\n    \n   \n    vec2 cam = getCamTrans(iTime);\n    mat2 camRot = getCamRot(iTime);\n    \n    // Threading a light through the moving pattern. The velocity is 1/6 units per second, and the\n    // zoom factor is 8, so considering that the angular segment velocity is half that of the camera\n    // velocity... I'm just going to take a few guesses... Can't believe that actually worked. Yay. \n    // We're doing physics. :D\n    vec2 lp = vec2(0) - vec2(0, cam.y - 2./8.); // 2./texMapScale.\n    vec2 rd;\n    \n \n    // Rotating and moving the canvas. A 2D \"to\" and \"from\" setup would be better, but this\n    // will do for the purpose of the demonstration.\n    uv = uv*camRot - cam;\n    \n    // Making a copy of the rotated coordinates for later use.\n    vec2 rUV = uv;\n    \n    float sf = 1./iResolution.y*4.;\n    float grid = gridField(uv*2.);\n    \n    \n    #ifndef DISTANCE_FIELD_ONLY\n \n    // Number of samples and the number of reflective bounces. As you can see, there are\n    // far fewer than you'd expect.\n    const int SAMPLES = 4;\n    const int BOUNCES = 2;\n    \n    // Sample loop.\n    for (int i =0; i < SAMPLES; i++){\n        \n        float fi = float(i);\n        \n        // Reflection factor: Basically, this controls the the reflected amount. Normally,\n        // you'd have reflection coefficients for each object material, etc, but we're \n        // trying to keep things simple, so this will simply be reduced globally per \n        // reflective iteration... It's not that important. :)\n        float refFactor = 1.; \n        \n        // Random time.\n        float fTm = fract(iTime) + fract(float(iFrame)*.01);\n        \n        // The initial jittered ray origin or camera point for this sample.\n        vec2 ro = uv + (hash22(uv + fi + fTm + .35) - .5)/iResolution.y*1.;\n        \n        // The initial random unit direction ray for this sample.\n        float ti = (fi + hash21(uv + fi + fTm))*6.2831/float(SAMPLES);\n        rd = vec2(cos(ti), sin(ti));\n        \n        // The sample color.\n        vec3 sCol = vec3(0);\n        \n        // Bounce loop.\n        for(int j = 0; j<BOUNCES; j++){\n            \n            // Bounce trace.\n            float t = trace(ro, rd);\n            vec2 svID = gIP; // Saving the ID here.\n            \n            float fj = float(j);\n            vec3 rCol = vec3(0);\n            vec2 sp = ro + rd*t;\n            \n            if(t<FAR){\n                \n                // The randomly distributed unit direction vector: For 2D stuff, this seems \n                // to be the accepted way to go about it. Basically it's just a normalized\n                // vector in a random circular direction, which makes sense on a 2D plane.\n                float fij = fi*float(BOUNCES) + fj;\n                float tij = (fij + hash21(uv + fij + fTm))*6.2831/float(SAMPLES*BOUNCES);\n                vec2 rndRD = vec2(cos(tij), sin(tij));\n                \n                // Basic lighting stuff... I'm not sure it makes perfect sense in a 2D \n                // environment, but it'll do.\n                //\n                vec2 sn = nr(sp); // Normal.\n                vec2 ld = (lp - sp); // Light direction.\n                float lDist = length(ld); // Light distance.\n                ld /= lDist; // Normalizing.\n                float udif = dot(ld, sn); // Unsigned diffuse value.\n                float spec = pow(max(dot(reflect(ld, sn), rd), 0.), 32.); // Specular.\n\n                // You could do much fancier things with the object ID, but\n                // I'm simply giving the rails different colors.\n                vec3 oCol = svID.x < .5? vec3(.25, .5, 1) : vec3(1.5, .3, .15);\n                 \n                // No textures, but you could have them.\n                //vec3 tx = texture(iChannel2, uv).xyz; tx *= tx;\n                //tx = smoothstep(.0, .5, tx);\n                \n                \n                 \n                // Sending a ray from the hit point out toward the light.\n                // Some people might call it shadowing. :)\n                #ifdef LIGHT_TRACE\n                vec3 dL = vec3(0);\n                //vec3 shad = vec3(1);\n                if(udif>0.){\n                    float maxDist = lDist;//lDist; // FAR.\n                    // Lamest cone sampled light ray ever. :)\n                    vec2 cnLD = normalize(mix(ld, rndRD, .05));\n                    float rt = lightTrace(sp + sn*DELTA*2., cnLD, maxDist);\n                    //float rt = shadowTrace(sp + sn*DELTA*2., cnLD);\n                    \n                    // If we've reached the light, light things up. It's\n                    // slightly different to the shadowed approach which \n                    // dictates that you darken things if you hit an object.\n                    if(rt >= maxDist - DELTA) {\n                        dL += pow(udif, 4.)*2.5;///(1. + rt*rt*4.);//\n                         \n                    }\n                    //if(rt < maxDist) shad = vec3(0);\n                }\n                #endif\n                \n                \n                // Shading and texture option. Not used.\n                //float sh = max(.2 - dfIJ/.01, 0.);\n                //oCol = mix(oCol, oCol*tx, (1. - step(0., -(dfIJ))));\n                \n                \n                #ifdef LIGHT_TRACE\n                // Apply the light traced light. Nicer, but more expensive. By the way,\n                // this is all fake lighting. The specular is there because I was bored,\n                // and it looked OK, so don't take any of this at face value.\n                rCol = oCol*(dL + spec); \n                #else\n                // Just some diffuse and specular light.\n                float dfIJ = map(sp);\n                rCol = oCol*(pow(max(udif, 0.), 4.)*2. + spec);\n                rCol *= 1. - smoothstep(0., .0005, dfIJ)*.75; // Darkenging the background\n                #endif\n               \n                // Light attenuation.\n                rCol *= 4./(1. + lDist*lDist*4.);\n                \n                \n                #ifdef SHOW_LIGHT\n                // Displaying the moving light.\n                rCol = mix(rCol, rCol*5., 1. - smoothstep(0., sf*4., length(uv - lp)));\n                #endif\n                \n              \n                // Mostly reflective, but adding in a little roughness.\n                rd = normalize(mix(reflect(rd, sn), rndRD, .05)); \n\n                // Updating the ray origin to the hit point, then bumping the ray\n                // off the surface to avoid self collisions.\n                ro = sp + sn*DELTA*2.;\n\n            }\n            \n            #ifdef SHOW_GRID\n            // Display the grid as a background pattern.\n            rCol = mix(rCol, rCol*2., (1. - smoothstep(0., sf*3., grid - .002))*.5);\n            rCol = mix(rCol, vec3(0), (1. - smoothstep(0., sf, grid))*.9);\n            #endif\n            \n             \n            // Blending in the bounce color. Additive blending is an option, but\n            // I'm mixing.\n            sCol = mix(sCol, rCol, 1./float(1 + j)*refFactor);\n            //lCol += rCol;\n            refFactor *= .9;\n            \n            if(t>FAR-DELTA) break;\n           \n        }\n         \n        \n        // Technially, this should be capped to one, but I wanted to really exaggerate\n        // the effect.\n        col += min(sCol, 2.);\n        \n        \n    }\n    \n    // Divide by the sample number.\n    col /= float(SAMPLES); \n    \n    // If you cap the reflective colors, this isn't necessary. You might also choose\n    // not to cap them, then cap here, etc. Too many choices. :)\n    //col = min(col, 1.);\n \n    \n    \n    #ifdef TEMPORAL_REPROJECTION\n    // Camera reprojection. This is basically the crux of the example, and as you \n    // can see, it's not that involved. In essence, we're calculating where we \n    // believe the previous frame should be placed on the sceen in relation to the\n    // new one -- Effectively, we'd like to place it directly under the new one. \n    // To do that, we index into the stored buffer at the current position minus \n    // the frame to frame camera difference.\n    //\n    // On a side note, I think the calculations are correct, but it's been a while \n    // since I've done this, so if there's something that doesn't look quite right,\n    // it probably isn't... And feel free to let me know that. It's the only way\n    // I'll learn not to be stupid. :D\n    \n    // Recalculating the UV coordinates.\n    uv = (fragCoord - iResolution.xy*.5)/iResolution.y;\n   \n    // Frame to frame camera translation difference.\n    vec2 camDelta = getCamTrans(iTime - iTimeDelta);\n    vec2 camOffs = -(camDelta - cam);\n    \n    // Frame to frame camera rotation difference.\n    mat2 rotDelta = getCamRot(iTime - iTimeDelta);\n    vec2 rotOffs = -(rotDelta*uv - camRot*uv);\n    \n    \n    // Offsetting the UV coordinates accordingly, then projecting to the current\n    // screen coordinates, which have to range from zero to one along X and Y.\n    vec2 cuv = (uv - rotOffs - camOffs)*vec2(iResolution.y/iResolution.x, 1)  + .5;\n    \n    // Two differently rotated-translated rectangles are more than likely not going\n    // to line up, so you need to check boundaries. You might note a bit of smudging\n    // on the screen borders. Usually, your offscreen buffer would be larger, or the\n    // onscreen buffer will be smaller. Basically, I could put a screen border around\n    // everything, but I don't think it's that noticeable.\n    if(cuv.x<0. || cuv.x>1.) cuv.x = uv.x*iResolution.y/iResolution.x + .5;\n    if(cuv.y<0. || cuv.y>1.) cuv.y = uv.y + .5;\n    vec3 tCol = texture(iChannel1, cuv).xyz;\n    \n    \n    // Mixing in the new frmae with the previous one. In fact, we're cycling about\n    // 8 screens. This effectively gives you 8 times the sampling. So, if your \n    // original sample count is just 8, this would boost it to 64. The reason you\n    // don't go too high is that temporal reprojection is just an estimation, so\n    // eventually, temporal screen lag will catch up with you.\n    const float totTimeFrames = 8.;\n    col = mix(tCol, col, 1./totTimeFrames);\n    \n    #endif\n    \n    #else\n    \n    // Display the distanc field only.\n    \n    // For anyone interested in the distance field map itself, uncomment the\n    // DISTANCE_FIELD_ONLY option, and you'll see that it's nothing more than \n    // a moring Voronoi texture or moving Voronoi Truchet arrangement.\n    //\n    vec3 txM = tx(iChannel0, rUV/4.).xyz;\n    col = vec3(0) + txM.x*repSc/2.;\n    #ifdef SHOW_GRID\n    col = mix(col, col*2., (1. - smoothstep(0., sf*3., grid - .0035))*.5);\n    col = mix(col, vec3(0), (1. - smoothstep(0., sf, grid))*.9);\n    #endif \n    \n    float sh = max(.65 - txM.x*repSc*4., 0.);\n    vec3 tCol = txM.y<.5? vec3(.15, .4, 4) : vec3(4, .4, .1);\n    \n    col = mix(col, vec3(0), (1. - smoothstep(0., sf, txM.x*repSc/2. - .03)));\n    col = mix(col, tCol*sh, (1. - smoothstep(0., sf, txM.x*repSc/2.)));\n    #endif \n    \n    // Output to the buffer.\n    fragColor = vec4((max(col, 0.)), 1);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "cubemap",
                        "id": 41,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/a//media/previz/cubemap00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            }
        ],
        "ver": "0.1"
    }
}