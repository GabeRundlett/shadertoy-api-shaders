{
    "Shader": {
        "info": {
            "date": "1575270525",
            "description": "rendering a sphere in ~330 characters (and lots of comments)",
            "flags": 0,
            "hasliked": 0,
            "id": "WsySDt",
            "likes": 13,
            "name": "Tiny Annotated Raymarcher",
            "published": 3,
            "tags": [
                "raymarcher",
                "comments",
                "guide",
                "annotated"
            ],
            "usePreview": 0,
            "username": "blackle",
            "viewed": 635
        },
        "renderpass": [
            {
                "code": "//CC0 1.0 Universal https://creativecommons.org/publicdomain/zero/1.0/\n//To the extent possible under law, Blackle Mori has waived all copyright and related or neighboring rights to this work.\n\n//a shader is a program that is executed for each pixel on the screen.\n//there is no communication between pixels, all the shader knows about\n//are the current pixel coordinate, and some global variables which are\n//the same for each pixel (for example, the time.)\n\n//in the purest sense, you can imagine a shader as just a function that\n//takes in a pixel coordinate and outputs its colour. that's all it does.\n\n//in this example, we will make a simple raymarcher to render a sphere!\n\n//here is the main function for the shader. it takes in the current pixel\n//coordinate, \"fragCoord\", and outputs the colour that this pixel should\n//be, \"fragColor\". other functions can be defined, but they must be called\n//from this mainImage function in order to do anything.\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    //in order to make our raytracer, we must map the pixel coordinates\n    //to the range (-1, 1). this will make it easier to generate our\n    //camera. the following code sets uv to be the mapped coordinates\n    vec2 uv = (fragCoord-iResolution.xy*0.5)/iResolution.y;\n\n    //next, we have to define the camera. to understand how we do this,\n    //I'd like to first explain what 3d vectors actually mean in a shader.\n    //a 3d vector has the type 'vec3'. you may have noticed there are vec2\n    //and vec4s already referenced in this code. as you might imagine,\n    //these are 2d vectors and 4d vectors respectively.\n    \n    //a vec3 can represent anything. they can be used to define a point in\n    //3d space, an RGB colour, or even the direction an object is pointing.\n    //it is up to us to choose how we want to use these vectors.\n    \n    //so, how do we define a camera? the first thing we want to know is where\n    //in space the camera is positioned. we can use a vec3 for this, and use\n    //it to define the 3d coordinates of the camera. I like to call this\n    //vector 'init', because it is the initial point.\n    vec3 init = vec3(-4, 0, 0);\n    //why I placed our camera at this location will become clear later!\n    \n    //next we need to know what direction our camera is pointing. if you've\n    //ever looked inside a digital camera, you might know that there is a \n    //sensor with many light sensing pixels arranged in a grid. light\n    //from the outside enters though the camera lens, and depending on the\n    //direction it entered from, a particular pixel will receive that light.\n    \n    //to do 3d graphics, we can try to simulate how this camera works. we\n    //identify each pixel our our screen with a pixel on a virtual camera's\n    //sensor, then \"trace\" the path that light would have taken to reach that\n    //pixel. the direction light takes is defined by a vec3 whose length is 1.\n    \n    //to calculate this direction, we use the following piece of code:\n    vec3 cam = normalize(vec3(1, uv));\n    //you might be wondering, \"I thought vec3 takes three values?\" well yes,\n    //since uv is a vec2, then the vec3 type will take the two components from\n    //the vec2 and use them for the y and z coordinates. together with the \"1\"\n    //which is the x coordinate, that makes three components.\n    \n    //the normalize function divides the input vector by that vector's length,\n    //this gives us a vector of length 1, which we are using to define the\n    //camera's direction.\n    \n    //so, where is the camera pointing? the y and z coordinates define the\n    //location of a pixel on the the virtual camera's sensor, and the x coordinate\n    //defines the location of the focal point of the camera. by drawing a line\n    //from the origin to vec3(1, uv), we get the direction that light needs to\n    //travel to hit that pixel on the virtual sensor.\n    \n    //the camera is pointing along the x axis, in the positive direction. since\n    //we're starting at vec3(-4, 0, 0), this implies we are looking toward the\n    //origin. this is where we are going to place a sphere!\n    \n    //but how do we define a sphere? how do we define the scene we are looking at?\n    //we will use is a construct called a \"signed distance function.\" this is a\n    //function that takes a point in 3d space, and returns the shortest distance\n    //to the surface of the scene. let's write some code to explain how you can\n    //use such a function to do 3d graphics!\n    \n    //first thing we must do is define a point in 3d space which we will update\n    //inside a loop. this loop will make this point get closer and closer to the\n    //surface of the scene at every iteration. the initial value of this point will\n    //be the 'init' point for the camera.\n    vec3 p = init;\n    //this point will move along the camera ray as it gets closer and closer\n    //to the surface. and once it hits the surface, we can use its location to\n    //pick a suitable output colour. so far so good.\n    \n    //but what if the point never reaches the surface? what colour do we choose\n    //then? and worse, how can we tell if this has happened or not? well, we\n    //use a boolean for that. it will be true if the point actually hit something\n    //in our scene.\n    bool hit = false;\n    \n    //next, we define the bounds of this loop. the longer you make the loop,\n    //the more accurate the rendering will be. don't make this value too big,\n    //or your graphics card might hang! I like to use 100 iterations\n    for (int i = 0; i < 100; i++) {\n        //now we have reached the meat of this excersize. the first thing we\n        //must do is figure out how close the point is to the surface using\n        //our distance function. but I haven't defined it yet, so here is where\n        //we decide what to render.\n        \n        //I want to render a sphere. the signed distance function for a sphere\n        //at the origin is one of the simplest functions. you take the distance of\n        //the point to the origin, and subtract the radius. if the point is\n        //inside the sphere, the resulting distance will be negative. this is where\n        //the \"signed\" part of signed distance functions come in. I want a sphere\n        //of radius 1, so I will subtract 1.\n        float dist = distance(p, vec3(0.)) - 1.;\n        //this can also be written as length(p)-1, to save even more characters!\n        \n        //next, we need to determine if we're \"close enough\" to the surface. to do\n        //this we just compare distance to some small value. this \"small value\"\n        //is often referred to as \"epsilon\", which is a tradition from mathematics.\n        //I like to use 0.001 as my epsilon value.\n        if (abs(dist) < 0.001) {\n            //since we are close enough, we need to set our 'hit' value to true,\n            //and exit the loop prematurely.\n            hit = true;\n            break;\n        }\n        \n        //if we're not close enough, we need to figure out how to move our point\n        //closer. we have two constraints that we also have to deal with. the\n        //point cannot stray from the camera ray, if it does then the result will be\n        //completely meaningless. another problem is since we know the closest distance\n        //to the surface, we cannot move the point further than that distance in any\n        //direction, otherwise we may have jumped into the surface, or worse, jumped over\n        //some small object in our scene!\n        \n        //what we'll do is use both of these constraints as strengths. since we cannot\n        //stray from the line defined by cam, let's just move along this line by some\n        //distance. and since we know the scene is no less than dist units away, we can\n        //safely jump along the line by that value and be confident that we haven't missed\n        //any part of our scene.\n        p += dist*cam;\n        //and that's it for the body of the loop!\n    }\n    \n    //at the end of the loop, either hit is false meaning the point didn't reach the\n    //surface, or hit is true and p is the closest point on the surface to the camera.\n    //what I like to do when I write a shader is use the coordinates of the point as\n    //the RGB colour. however we have a problem, since p can be any coordinate in 3d\n    //space, but an RGB colour must be between 0 and 1. to get around this, I pass the\n    //point into sin, then normalize sin between 0 and 1. this tends to give a quick\n    //visual of the scene, and more complicated shading can come later.\n    \n    //for some reason, fragColor has four components. The first three coordinates are the\n    //red, green and blue channels. the fourth is unused by shadertoy, but I believe it is\n    //meant to be the alpha channel in general. we can set the rgb components individually\n    //and leave the last component unset.\n    \n    //I am going to use the ternary operator, which is `a ? b : c`. this means 'if a\n    //is true, then use the value b, otherwise use c.'\n    fragColor.rgb = hit ? sin(p)*0.5+0.5 : vec3(0.25);\n    //the vec3(0.25) defines the background colour. this makes the colour around the sphere\n    //a dark grey, which I think looks very pretty :3\n\n    //and that's it! if this interests you, I recommend playing around with some of the\n    //variables we chose. for example, the location of 'init'. you can move it around,\n    //and maybe use the 'iTime' variable, which gives the seconds that the shader has been\n    //running for, to change the position based on time. you could also change the radius\n    //of our sphere, or even (if you are feeling adventurous) figure out a way to rotate\n    //the camera around it.\n    \n    //thanks for reading!\n}",
                "description": "",
                "inputs": [],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            }
        ],
        "ver": "0.1"
    }
}