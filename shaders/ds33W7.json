{
    "Shader": {
        "info": {
            "date": "1677352107",
            "description": "Right click and drag to look\nWASD to move\nEsc: Reset camera",
            "flags": 48,
            "hasliked": 0,
            "id": "ds33W7",
            "likes": 2,
            "name": "Mandelbulb spheretracer",
            "published": 3,
            "tags": [
                "raymarching",
                "fractal",
                "spheretracing"
            ],
            "usePreview": 0,
            "username": "rmMinusR",
            "viewed": 185
        },
        "renderpass": [
            {
                "code": "/*\n\nMidterm: Fractal raymarcher\n'Image' tab\n\nJust a passthrough. Existed for testing the contents of the accumulation buffer, but\nit isn't necessary anymore. Set it up for buffer B.\n\nChannel setup:\n 0: buffer B\n\n*/\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    fragColor = texelFetch(iChannel0, ivec2(fragCoord), 0);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "/*\n\nMidterm: Fractal raymarcher\n'Buffer A' tab by Sean Sawyers-Abbott, with contributions from Robert Christensen\n\nThis shader is an accumulation buffer that manages the camera position and rotation.\n\nChannel setup:\n 0: self\n 1: keyboard\n\n*/\n\n// calculates the camera's movement in global space based on the user's inputs\nvec3 calcMovement()\n{\n    // gets the camera rotation\n    vec2 camRot = texelFetch(iChannel0, camRotInd, 0).xy;\n    \n    // local right axis\n    vec3 right = vec3(cos(camRot.y), 0.0, sin(camRot.y));\n    \n    // local forward axis\n    vec3 forward = vec3(sin(camRot.y) * cos(camRot.x),\n                                       -sin(camRot.x),\n                       -cos(camRot.y) * cos(camRot.x));\n    // global up axis\n    vec3 up = vec3(0., 1., 0.);\n    \n    // gets the user's key inputs to determine camera movement\n    // \"local\" 3D movement ranging -1 to 1 on each axis\n    vec3 keyInput = vec3( texelFetch(iChannel1, ivec2(KEY_D, 0), 0).r\n                         -texelFetch(iChannel1, ivec2(KEY_A, 0), 0).r,\n                          texelFetch(iChannel1, ivec2(KEY_W, 0), 0).r\n                         -texelFetch(iChannel1, ivec2(KEY_S, 0), 0).r,\n                          texelFetch(iChannel1, ivec2(SPACE, 0), 0).r\n                         -texelFetch(iChannel1, ivec2(LCTRL, 0), 0).r);\n    \n    // checks if the shift key is held to speed up camera movement\n    keyInput *= texelFetch(iChannel1, ivec2(SHIFT, 0), 0).r + 1.;\n    \n    // puts movement into the axes\n    right *= keyInput.x * moveSens;\n    forward *= keyInput.y * moveSens;\n    up *= keyInput.z * moveSens;\n    \n    // returns total movement\n    return right + forward + up;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    // gets current pixel\n    ivec2 posCoord = ivec2(fragCoord.x, fragCoord.y);\n    \n    // checks if the current pixel is where movement is stored\n    if (posCoord == camPosInd)\n    {\n        // initialization to default value: checks if on the first frame or escape is pressed\n        if (iFrame == 0 || texelFetch(iChannel1, ivec2(ESC, 0), 0).r == 1.)\n        {\n            // moves the camera outside of the fractal\n            // slightly offsets the XY position as for some reason\n            // if it's at 0.0, 0.0, then it will teleport into\n            // the fractal upon moving directly forwards\n            // or backwards upon start\n            fragColor = vec4(0.001, 0.001, 1.5, 0.);\n            //RC: this is because distance estimation messes up around (x=0, y=0)\n            //Take a look at the -Z tip of the fractal and you'll see it break down into pointclouds\n        }\n        else\n        {\n            // converts the sampled channel into a texture\n            vec4 camera = texelFetch(iChannel0, camPosInd, 0);\n            \n            // gets the length of the camera\n            float lenCam = length(camera);\n            \n            // checks if the camera's length is too far away\n            if (lenCam > 4.5)\n            {\n                // sets the camera to be inside the safe space\n                camera = camera / lenCam * 4.5;\n            }\n            \n            // gets the distance from the camera to the nearest point\n            float dist = signedDistance(camera);\n            \n            // calculates the movement while getting slower when it\n            // gets closer to the nearest point\n            fragColor = camera + vec4(calcMovement() * dist / iTimeDelta, 0.0);\n        }\n    }\n    \n    // checks if the current pixel is where mouse position is stored\n    if (posCoord == mouseInd)\n    {\n        // allows the user to control the camera's rotation with the mouse\n        fragColor.xy = iMouse.xy;\n    }\n    \n    // checks if the current pixel is where the camera's rotation is stored\n    if (posCoord == camRotInd)\n    {\n        // checks if escape is pressed\n        if (texelFetch(iChannel1, ivec2(ESC, 0), 0).r == 1.)\n        {\n            fragColor = vec4(0.);\n        }\n        else\n        {\n\t        // gets the camera's rotation\n\t        vec2 camRot = texelFetch(iChannel0, ivec2(camRotInd), 0).xy;\n\t        \n\t        // gets the mouse's x and y positions\n\t        vec2 deltaMouse = iMouse.xy - texelFetch(iChannel0, mouseInd, 0).xy;\n\t        \n\t        // ensures smooth, consistent rotation of the camera\n\t        if (length(deltaMouse) < 25.)\n\t        {\n\t            // rotates the camera at the given speed\n\t            camRot += deltaMouse.yx * mouseSens;\n\t        }\n\t        \n        \t// rotates the camera in the x direction and\n        \t// stops the camera from doing a full rotation up or down\n        \tfragColor.x = clamp(camRot.x, -90.*DEG2RAD, 90.*DEG2RAD);\n        \t\n        \t// rotates the camera in the y direction\n        \tfragColor.y = camRot.y;\n        }\n    }\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "keyboard",
                        "id": 33,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/presets/tex00.jpg"
                    },
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "/*\n\nMidterm: Fractal raymarcher\n'Buffer B' tab by Robert Christensen\n\nThis shader is responsible for rendering the scene.\n\nChannel setup:\n 0: buffer B\n 1: any cubemap for a background\n\n*/\n\n// CAMERA SETTINGS\n\nconst float viewportHeight = 2.0;\nconst float focalLength = 1.0;\n\n// BEGIN RAYMARCHER\n\n// March constructor\nMarch mk_March(in Ray ray) { March val; val.position = ray; val.closestApproach = MARCH_MAX_DIST; return val; }\n\n// Execute a single step of raymarching\nfloat march_step(inout March march) {\n    float d = signedDistance(march.position.origin);\n    march.position.origin += d*march.position.direction;\n    march.distanceMarched += d;\n    march.closestApproach = min(march.closestApproach, d);\n    return d;\n}\n\n// Perform a raymarch from the camera\nMarch cam_march(in Ray ray) {\n    March march = mk_March(ray);\n    //MARCH'S POPULATED VALUES: position.direction\n    \n    PointLight l = mk_PointLight(vec4(0.5,0.5,0,1), vec3(1), 16.);\n    \n    //March until we hit something, or run out of tries. In other words, correctly set the position\n    float d = MARCH_MAX_DIST; // temp var\n    bool hit, nohit;\n    do {\n        d = march_step(march);\n        ++march.iterations;\n        \n        //Exit condition: we didn't hit anything\n        nohit = march.distanceMarched > MARCH_MAX_DIST || march.iterations > MARCH_MAX_STEPS;\n        \n        //Exit condition: we hit something\n        hit = d < MARCH_HIT_THRESHOLD;\n    } while(!hit && !nohit);\n    \n    //Apply color and shading\n    if(hit) {\n        //MARCH'S POPULATED VALUES: position, distanceMarched, iterations, closestApproach\n\t\t\n        march.normal = normal(march.position.origin, normal_detail);\n        march.color = color(march);\n\n        //MARCH FULLY POPULATED\n        \n        //Apply lighting\n        march.color = lambert_light(l, march.color, march.position.origin, march.normal);\n    } else {\n        //We didn't hit anything. Set to transparent so layer can be blended\n        march.color = vec4(0,0,0,0);\n    }\n    \n    //Halo effect\n    //Helps emphasize surface details and edges in monotone \"caves\"\n    float halo = float(march.iterations)/96. - 0.1/max(march.closestApproach,1.);\n    halo = clamp(halo, 0., 1.);\n    march.color.rgb += vec3(1) * halo;\n    march.color.a = 1.-( (1.-march.color.a) * (1.-halo) );\n    \n    return march;\n}\n\n// END RAYMARCHER\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    // viewing plane (viewport) info\n    vec2 viewport, px_size, ndc, uv, resolutionInv;\n    float aspect;\n    \n    // setup\n    calcViewport(viewport, px_size, ndc, uv, aspect, resolutionInv,\n                 viewportHeight, fragCoord, iResolution.xy);\n    \n    // make ray for fragment\n    Ray ray;\n    calcRay(ray, viewport, focalLength,\n            texelFetch(iChannel0, camRotInd, 0).xy, texelFetch(iChannel0, camPosInd, 0).xyz);\n    \n    //Required so steps are the right distance\n    ray.direction = normalize(ray.direction);\n    \n    March march = cam_march(ray);\n    \n    alpha_blend(texture(iChannel1, ray.direction.xyz), clamp(march.color, 0., 1.), fragColor);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "cubemap",
                        "id": 23,
                        "published": 1,
                        "sampler": {
                            "filter": "mipmap",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "false",
                            "wrap": "clamp"
                        },
                        "src": "/media/a/793a105653fbdadabdc1325ca08675e1ce48ae5f12e37973829c87bea4be3232.png"
                    },
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer B",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 258
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "/*\n\nMidterm: Fractal raymarcher\n'Common' tab by Robert Christensen and Sean Sawyers-Abbott\n\nContents:\n - Parameters\n - Constants\n - Utils\n - Data structures\n - Lab 3 boilerplate\n - Lights\n    - Lambertian model\n - Mandelbulb\n\n*/\n\n// BEGIN PARAMETERS\n\n// Parameters for raymarching\nconst int MARCH_MAX_STEPS = 256;   // Maximum iteration cap. Must be something reasonable or WebGL will crash.\nconst float MARCH_MAX_DIST = 128.; // Distance for us to safely assume we haven't hit anything.\nconst float MARCH_HIT_THRESHOLD = 0.00001; // How close must we be to be considered a \"hit\"?\n\n// Mouse and keyboard sensitivity for controlling the camera\nconst vec2 mouseSens = vec2(-0.01, 0.01);\nconst float moveSens = 0.001;\n\n// Mandelbulb parameters\n\n//#define power (1.+mod(iTime/8., 1.)*8.)\nconst float power = 10.;\nconst float normal_detail = 0.004;\n\n// END PARAMETERS\n\n\n// BEGIN CONSTANTS\n\n// Coordinates where each piece of data is stored in accumulation buffer\nconst ivec2 camPosInd = ivec2(0,0);\nconst ivec2  mouseInd = ivec2(1,0);\nconst ivec2 camRotInd = ivec2(2,0);\n\n// Math constants\nconst float PI = 3.1415926535;\nconst float DEG2RAD = PI/180.;\n\n// Keycode constants for A, W, D, and S keys respectively\nconst int ESC = 0x1b;\nconst int SHIFT = 0x10;\nconst int LCTRL = 0x11;\nconst int SPACE = 0x20;\nconst int KEY_A = 0x41;\nconst int KEY_D = 0x44;\nconst int KEY_S = 0x53;\nconst int KEY_W = 0x57;\n\n// END CONSTANTS\n\n\n// BEGIN UTILS\n\n// Square\n#define GEN_DECLARE(genType) genType sq(in genType v) { return v*v; }\nGEN_DECLARE(int  ) GEN_DECLARE(ivec2) GEN_DECLARE(ivec3) GEN_DECLARE(ivec4)\nGEN_DECLARE(float) GEN_DECLARE( vec2) GEN_DECLARE( vec3) GEN_DECLARE( vec4)\n#undef GEN_DECLARE\n\n// \"Un-reserve\" the this keyword\n#define this _this\n\n// Blend layers based on transparency\nvoid alpha_blend(in vec4 back, in vec4 front, out vec4 result) {\n    result = vec4( mix(back, front, front.a).rgb, 1.-( (1.-back.a)*(1.-front.a) ) );\n}\n\n// END UTILS\n\n\n// BEGIN DATA STRUCTURES\n\nstruct Ray {\n    vec4 origin, direction;\n};\n\nstruct March {\n    //We need approach direction as well for light view_v -> halfway\n    //(BP only) because camera will be moving\n    Ray position;\n    \n    //Saves a normalize() call\n    float distanceMarched;\n    int iterations;\n    \n    //Traditional outputs\n    vec4 normal;\n    vec4 color;\n    \n    //Allows for halos and other cool effects\n    //Ideally would be a double\n    float closestApproach;\n};\n\n// END DATA STRUCTURES\n\n\n// BEGIN LAB 3 BOILERPLATE\n// These snippets were copy-pasted from the assignment main page\n// calcViewport() modified by RC for use in SSAA (currently unused for performance reasons)\n// calcRay() modified by SSA for camera transformation\n\n// Calculate the coordinate on the viewing plane\nvoid calcViewport(out vec2 viewport, out vec2 px_size, out vec2 ndc, out vec2 uv,\n                  out float aspect, out vec2 resolutionInv,\n                  in float viewportHeight, in vec2 fragCoord, in vec2 resolution)\n{\n    // inverse (reciprocal) resolution = 1 / resolution\n    resolutionInv = 1.0 / resolution;\n    \n    // aspect ratio = screen width / screen height\n    aspect = resolution.x * resolutionInv.y;\n\n    // uv = screen-space coordinate = [0, 1) = coord / resolution\n    uv = fragCoord * resolutionInv;\n\t\n    // ndc = normalized device coordinate = [-1, +1) = uv*2 - 1\n    ndc = uv * 2.0 - 1.0;\n    \n    // viewport: x = [-aspect*h/2, +aspect*h/2), y = [-h/2, +h/2)\n    vec2 rhsCoeff = vec2(aspect, 1.0) * (viewportHeight * 0.5);\n    viewport = ndc * rhsCoeff;\n    px_size = resolutionInv * 2. * rhsCoeff; //Derived from UV and NDC\n    \n}\n\n// Build a ray for the current pixel\nvoid calcRay(out Ray ray, in vec2 viewport, in float focalLength,\n             in vec2 camRot, in vec3 camPos)\n{\n    // ray origin relative to viewer is the origin\n    ray.origin = vec4(camPos, 1.0); //SSA: move camera\n\n    // ray direction relative to origin is based on viewing plane coordinate\n    ray.direction = vec4(viewport.x, viewport.y, -focalLength, 0.0);\n    \n    //SSA: rotate camera rays\n    mat3 xRot = mat3(vec3(1, 0, 0),\n                     vec3(0, cos(camRot.x), -sin(camRot.x)),\n                     vec3(0, sin(camRot.x),  cos(camRot.x)));\n    \n    mat3 yRot = mat3(vec3(cos(camRot.y), 0, sin(camRot.y)),\n                     vec3(0, 1, 0),\n                     vec3(-sin(camRot.y), 0, cos(camRot.y)));\n    \n    ray.direction.xyz = yRot * xRot * ray.direction.xyz;\n}\n\n// END LAB 3 BOILERPLATE\n\n\n// BEGIN LIGHTS\n\nstruct PointLight {\n    vec4 pos;\n    vec4 color; //W/A used as intensity\n};\n\n// Initialize a PointLight\nPointLight mk_PointLight(in vec4 center, in vec3 color, in float intensity) {\n    PointLight val;\n    val.pos = vec4(center.xyz, 1.);\n    val.color.rgb = color;\n    val.color.a = intensity;\n    return val;\n}\n\n// Attenuation coefficient\nfloat attenuation_coeff(in float d, in float intensity) {\n    return 1./sq(d/intensity+1.);\n}\n\n// BEGIN LAMBERTIAN MODEL\n\n// Lambertian diffuse coefficient\n// BOTH INPUTS MUST BE NORMALIZED\nfloat lambert_diffuse_coeff(in vec4 light_ray_dir, in vec4 normal) {\n    return dot(normal, light_ray_dir);\n}\n\n// Lambertian diffuse coefficient\nfloat lambert_diffuse_intensity(in PointLight light, in vec4 pos, in vec4 nrm) {\n    vec4 light_vector = light.pos-pos;\n    float lv_len = length(light_vector);\n    return lambert_diffuse_coeff(light_vector/lv_len, nrm) * attenuation_coeff(lv_len, light.color.a);\n}\n\n// Apply the Lambertian lighting model (only supports one light)\nvec4 lambert_light(in PointLight light, in vec4 color, in vec4 pos, in vec4 nrm) {\n    return vec4( lambert_diffuse_intensity(light, pos, nrm) * color.rgb * light.color.rgb, color.a);\n}\n\n// END LAMBERTIAN MODEL\n\n// END LIGHTS\n\n// BEGIN MANDELBULB\n\n// Mandelbulb distance estimation. We vaguely understand it but it isn't our code--we just optimized it.\n// http://blog.hvidtfeldts.net/index.php/2011/09/distance-estimated-3d-fractals-v-the-mandelbulb-different-de-approximations/\n// Via: https://www.youtube.com/watch?v=Cp5WWtMoeKg\nfloat signedDistance(in vec4 position) {\n    vec3 z = position.xyz;\n    float dr = 1.0;\n    float r = 0.0;\n    \n    for (int i = 0; i < 15; ++i) {\n        r = length(z);\n\t\t\n        // Escape check\n        if (r<=2.) {\n            // convert to polar coordinates\n            float theta = acos(z.z/r);\n            float phi = atan(z.y,z.x);\n            dr =  pow( r, power-1.0)*power*dr + 1.0;\n\n            // scale and rotate the point\n            float zr = pow( r,power);\n            theta = theta*power;\n            phi = phi*power;\n\n            // convert back to cartesian coordinates\n            z = zr*vec3(sin(theta)*cos(phi), sin(phi)*sin(theta), cos(theta));\n            z+=position.xyz;\n        } else break;\n    }\n    \n    return 0.5*log(r)*r/dr;\n}\n\n// Approximates the normal of the surface\n// Offset should probably be dynamic based on view distance, otherwise the image will look noisy.\n// Adapated from http://blog.hvidtfeldts.net/index.php/2011/08/distance-estimated-3d-fractals-ii-lighting-and-coloring/\nvec4 normal(in vec4 glob_pos, in float offset) {\n    vec4 xDir = vec4(offset, 0., 0., 0.);\n    vec4 yDir = vec4(0., offset, 0., 0.);\n    vec4 zDir = vec4(0., 0., offset, 0.);\n    return vec4(normalize(vec3(signedDistance(glob_pos+xDir)-signedDistance(glob_pos-xDir),\n\t\t                       signedDistance(glob_pos+yDir)-signedDistance(glob_pos-yDir),\n\t\t                       signedDistance(glob_pos+zDir)-signedDistance(glob_pos-zDir))), 0.);\n}\n\n// Simple bit of coloring\n// Makes more sense with this architecture than orbit trapping\nvec4 color(in March march) {\n    return vec4( march.normal.xyz*0.5+0.5, 1. );\n}\n\n// END MANDELBULB",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            }
        ],
        "ver": "0.1"
    }
}