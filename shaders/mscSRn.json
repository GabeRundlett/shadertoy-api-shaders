{
    "Shader": {
        "info": {
            "date": "1680093606",
            "description": "A realtime path traced hexagon prism grid traversal using IQ's temporal reprojection routine and some subtle denoising to give the appearance of a higher sample count.",
            "flags": 32,
            "hasliked": 0,
            "id": "mscSRn",
            "likes": 88,
            "name": "Multisample Cell Traversal",
            "published": 3,
            "tags": [
                "grid",
                "raycasting",
                "global",
                "illumination",
                "hexagon",
                "pathtracing",
                "city",
                "multisample",
                "denoise",
                "reprojection",
                "emitter"
            ],
            "usePreview": 0,
            "username": "Shane",
            "viewed": 5529
        },
        "renderpass": [
            {
                "code": "/*\n\n    Multisample Cell Traversal\n    --------------------------\n\n    See \"Buffer A\" for an explanation.\n\n*/\n\n\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord){\n\n\n    // Coordinates.\n    vec2 uv = fragCoord/iResolution.xy;\n    \n    // Retrieving the stored color.\n    vec4 col = texture(iChannel0, uv);\n    \n    /*\n    // Chromatic aberration.\n    // Pixel spread. Aesthetically, it's difficult to decide how you want to deal\n    // with different resolutions. You'll either want a specific pixel shift or a\n    // percentage shift based on physical screen distances.\n    float px = min(2./450., 2./iResolution.y);//max(2./450., 2./mix(450., maxRes, .333));//\n    vec4 r = texture(iChannel0, uv - vec2(px, 0)); // Red shifted to the right.\n    vec4 g = texture(iChannel0, uv); // Green in the middle.\n    vec4 b = texture(iChannel0, uv - vec2(-px, 0)); // Blue shifted to the left.\n    vec4 col = vec4(r.x, g.y, b.z, 1); // Plain old primary RGB.\n    //vec4 col = vec4(g.x, b.y, r.z, 1); // Alternate secondary RGB, or whatever you call it. :)\n    */\n\n    \n    // Subtle vignette.\n    //col *= pow(16.*uv.x*uv.y*(1. - uv.x)*(1. - uv.y) , 1./32.);\n    \n  \n    // Rough gamma correction and screen presentation.\n    fragColor = pow(max(col, 0.), vec4(1./2.2));\n    \n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "/*\n\n    Multisample Cell Traversal\n    --------------------------\n    \n    I've often wondered why some of us, incuding myself, like that noisy path \n    traced aesthetic whilst others don't. I'm not sure why, but for me, I \n    associate noisy light based stuff with elite democoders from the past.\n\n    Either way, I'd imagine virtually all of us prefer crisp clean path \n    traced imagery over the noisy ones. Unfortunately, the former is nearly\n    impossible to achieve in realtime inside a pixelshader environment for \n    anything but the simplest of scenes.\n    \n    This particular example represents a compromise. The results wouldn't pass \n    the static imagery test, or the intersting scene test. :D However, it's \n    pretty good for something produced under realtime constraints. By the way, \n    I have an asymmetric quad traversal rendered in the same style that I'll \n    post pretty soon.\n    \n    In order to achieve this, I've performed a cell by cell hexagon traversal \n    and rendered just six samples with three bounces each. That amount of work \n    is not ideal, but an average machine should be able to handle it. Six \n    samples produces a pretty noisy result, so I've applied IQ's temporal \n    reprojection code to give the impression that there are eight times that, \n    so 48 samples in all -- The trade-off is some slight temporal blurring. \n    However, even that isn't quite enough to clean up the noise, so I've \n    included a denoising pass in Buffer B.\n    \n    Overall, the image isn't perfect, but it's not too bad for just a few \n    samples. The reason someone would go through all that trouble is a mixture\n    of academic curiosity and to produce lighting in a scene that is not\n    possible to recreate using more common realtime Blinn-Phong related \n    lighting methods. \n    \n    \n    \n    Other examples:\n    \n    \n    // Beautiful, concise example.\n    Small Pathtracer - fizzer\n    https://www.shadertoy.com/view/ll3SDr\n    \n    // Path tracing a heap of boxes in realtime with the help of camera\n    // reprojection -- It's one of IQ's many understated examples that \n    // does something amazing.\n    Some boxes - iq\n    https://www.shadertoy.com/view/Xd2fzR\n    \n    // One of my favorite realtime path tracing examples.\n    20211031_Shader Royale (0b5vr) - 0b5vr\n    https://www.shadertoy.com/view/7td3zn\n\n\n*/\n\n\n// The default hexagon grid. Commenting it out will give rectangles.\n#define HEXAGON\n\n// Unfortunately, if you have a slow machine IQ's temporal reprojection option\n// will usually result in blur. Regular accumulation might work, but you'll \n// probably have to use straight samples (BUFF_ACCUM 0).\n// Buffer accumulation style:\n// 0: No accumulation -- Noisy, sharper picture, but with no blur. \n// 1: Regular accumulation with no reprojection -- A mixture.\n// 2: Temporal reprojection. -- Smoother for faster machines.\n#define BUFF_ACCUM 2\n\n\n// Far plane. I've kept it close.\n#define FAR 25.\n\n\n///////////////\n\n// Standard 2D rotation formula.\nmat2 rot2(in float a){ float c = cos(a), s = sin(a); return mat2(c, -s, s, c); }\n\n\n// Random seed value.\nvec2 seed = vec2(.1023, .2157);\n\n/*\n// Basic random function. Not a lot of thought was put into it. Using one of\n// the more reliable positive integer based ones might be better... Common sense\n// would dictate that integers work faster, but on a GPU, I'm not really sure.\nvec2 hash22() {\n    \n    //seed += vec2(.723, .647);\n    //return texture(iChannel2, seed*43266.1341).xy;\n    \n    seed = fract(seed + vec2(.723, .647));\n    return fract(sin(vec2(dot(seed.xy, vec2(12.989, 78.233)), dot(seed.xy, vec2(41.898, 57.263))))\n                    *vec2(43758.5453, 23421.6361));\n}\n*/\n \n// A slight variation on a function from Nimitz's hash collection, here: \n// Quality hashes collection WebGL2 - https://www.shadertoy.com/view/Xt3cDn\nvec2 hash22(){\n\n    // I should probably use a \"uvec2\" seed, but I hacked this from an old\n    // example. I'll update it later.\n    seed = fract(seed + vec2(.7123, .6457));\n    uvec2 p = floatBitsToUint(seed);\n    \n    // Modified from: iq's \"Integer Hash - III\" (https://www.shadertoy.com/view/4tXyWN)\n    // Faster than \"full\" xxHash and good quality.\n    p = 1103515245U*((p>>1U)^(p.yx));\n    uint h32 = 1103515245U*((p.x)^(p.y>>3U));\n    uint n = h32^(h32>>16);\n\n    uvec2 rz = uvec2(n, n*48271U);\n    // Standard uvec2 to vec2 conversion with wrapping and normalizing.\n    return vec2((rz>>1)&uvec2(0x7fffffffU))/float(0x7fffffff);\n}\n \n\n/*\nvec3 rnd23(){\n\n    seed = fract(seed + vec2(.7423, .6047));\n    uvec2 x = floatBitsToUint(seed);\n    uint n = baseHash(x);\n    uvec3 rz = uvec3(n, n*16807U, n*48271U);\n    return vec3((rz >> 1) & uvec3(0x7fffffffU))/float(0x7fffffff);\n}\n*/\n\n// IQ's vec2 to float hash.\nfloat hash21(vec2 p){ return fract(sin(mod(dot(p, vec2(27.619, 57.583)), 6.2831))*43758.5453); }\n\n// IQ's vec3 to float hash.\n//float hash31(in vec3 p){ return fract(sin(dot(p, vec3(91.537, 151.761, 72.453)))*435758.5453); }\n\n\n \n// A nice random hemispherical routine taken out of one of IQ's examples.\n// The routine itself was written by Fizzer.\nvec3 cosDir( in float seed, in vec3 n){\n\n    vec2 rnd = hash22();\n    float u = rnd.x;\n    float v = rnd.y;\n    \n    // Method 1 and 2 first generate a frame of reference to use with an arbitrary\n    // distribution, cosine in this case. Method 3 (invented by fizzer) specializes \n    // the whole math to the cosine distribution and simplfies the result to a more \n    // compact version that does not depend on a full frame of reference.\n\n    // Method by fizzer: http://www.amietia.com/lambertnotangent.html\n    float a = 6.2831853*v;\n    u = 2.*u - 1.;\n    return normalize(n + vec3(sqrt(1. - u*u)*vec2(cos(a), sin(a)), u));\n    \n}\n\n////////////////\n\n// The following is based on John Hable's Uncharted 2 tone mapping, which\n// I feel does a really good job at toning down the high color frequencies\n// whilst leaving the essence of the gamma corrected linear image intact.\n//\n// To arrive at this non-tweakable overly simplified formula, I've plugged\n// in the most basic settings that work with scenes like this, then cut things\n// right back. Anyway, if you want to read about the extended formula, here\n// it is.\n//\n// http://filmicworlds.com/blog/filmic-tonemapping-with-piecewise-power-curves/\n// A nice rounded article to read. \n// https://64.github.io/tonemapping/#uncharted-2\nvec4 uTone(vec4 x){\n    return ((x*(x*.6 + .1) + .004)/(x*(x*.6 + 1.)  + .06) - .0667)*1.933526;    \n}\n\n\n\n// Polygon scale: Smaller scales mean smaller grid shapes, thus more of\n// them. Sometimes, people (including myself) will confuse matters and \n// use the inverse number. :)\n#ifdef HEXAGON\n#define sqrt3 1.7320508 // sqrt(3.).\n// Hexagons will only work with the \"1 to sqrt3\" ratio -- Scaling is fine though. \n// At some stage, I'll arrange for anything to work, but it's not trivial.\nconst vec2 s = vec2(1, sqrt3)/2.; \n#else\n// Rectangle dimensions. Any numbers should work. Obviously, vec2(1)\n// will produce squares.\nconst vec2 s = vec2(2, 1)/3.; \n#endif\n\n // Ray origin, ray direction, point on the line, normal. \nfloat rayLine(vec2 ro, vec2 rd, vec2 p, vec2 n){\n   \n   // This it trimmed down, and can be trimmed down more. Note that \n   // \"1./dot(rd, n)\" can be precalculated outside the loop. However,\n   // this isn't a GPU intensive example, so it doesn't matter here.\n   return dot(p - ro, n)/dot(rd, n);\n\n}\n/*\n// Grid cell function.\nvec2 gridID(vec2 p){\n    // Using the rectangle's center position for the ID. \n    return floor(p/s) + .5;\n\n}\n*/\n\n// Sign function without the zero, which can cause problems for some routines.\nvec3 sign2(in vec3 p){ return vec3(p.x<0.? -1 : 1, p.y<0.? -1 : 1,  p.z<0.? -1 : 1); }\n//vec2 sign2(in vec2 p){ return vec2(p.x<0.? -1 : 1, p.y<0.? -1 : 1); }\n \n\n\n// Grid cell function.\nvec2 gridID(vec2 p){\n\n    \n    // Returns the cell center position-based IDs for a hexagon \n    // grid, offset rectangle grid and regular rectangle grid.\n\n    #ifdef HEXAGON\n    \n    // Hexagons.  \n    //\n    vec4 hC = floor(vec4(p/s, p/s - vec2(.5, sqrt3/3.))) + .5;\n    vec4 h = vec4(p - hC.xy*s, p - (hC.zw + .5)*s);\n    return dot(h.xy, h.xy)<dot(h.zw, h.zw) ? hC.xy : hC.zw + .5;\n    \n    #else\n    \n    // Rectangles.   \n    //    \n \n    // Rectangles with no offset. \n    vec2 ip = floor(p/s) + .5;\n    return ip;\n     \n    \n    #endif\n  \n}\n\nfloat h(vec2 p){\n\n    //return hash21(p)*4.;\n    \n    // Only one texture read.\n    // I'm using the Shadertoy noise texture, which I'm assuming is already in\n    // linear form, so I'm not performing the rought sRGB to linear conversion... \n    // I don't know for sure, but it doesn't matter here.\n    vec3 tx = texture(iChannel0, p/64.).xyz;  \n    //tx *= tx; // Rough sRGB to linear conversion.\n    // Greyscale height. Using \"tx.x\" would be OK, too.\n\tfloat f = dot(tx, vec3(.299, .587, .114));\n    //float f2 = f;\n \n    return f*6.;\n    //return floor(f*48.)/8.;//floor((f*8. + f2*4.)*8.)/8.;\n    \n    //return (sin(f*6.2831*4. + iTime)*.5 + .5)*1.;\n\n}\n \n// A standard square cell by cell traversal. Not optimized enough for path tracing\n// purposes, but it's reasonable quick otherwise.\nvec4 raycast(vec3 ro, vec3 rd){\n   \n    // Initial result.\n    vec4 res = vec4(FAR, 0, 0, 0);\n\n    #ifdef HEXAGON\n    // Rectangle normals: Any two will do. By the way, there's nothing\n    // stopping you from declaring all four normals for all surrounding\n    // walls, but since you know only two will be in front of the\n    // direction ray at any given time, it makes sense to only choose\n    // two.\n    //\n    // Declare two normals. Any side by side ones will do.\n    vec2 i1 = vec2(.5, .5); // Right forward face index.\n    vec2 i2 = vec2(1, 0); // Right face index.\n    vec2 i3 = vec2(-.5, .5); // Left forward face index.\n    vec2 n1 = vec2(.5, sqrt3/2.); // Right forward.\n    vec2 n2 = vec2(1, 0); // Right face normal.\n    vec2 n3 = vec2(-.5, sqrt3/2.); // Left\n    \n    // If the cell wall is behind the ray (or the ray is facing the opposing cell\n    // wall, if you prefer), use the normal index from the back cell wall. This \n    // trick is possible because of the rectangle symmetry. As an aside, for \n    // anyone who doesn't know, dotting the direction ray with the face normal \n    // is something you do in software engines for back face culling.\n    float d1 = dot(rd.xz, n1), d2 = dot(rd.xz, n2), d3 = dot(rd.xz, n3); \n    // As discussed above.\n    if(d1<0.) { i1 *= -1.; n1 *= -1.; }\n    if(d2<0.) { i2 *= -1.; n2 *= -1.; }\n    if(d3<0.) { i3 *= -1.; n3 *= -1.; }\n    #else\n    // Rectangle normals: Any two will do. By the way, there's nothing\n    // stopping you from declaring all four normals for all surrounding\n    // walls, but since you know only two will be in front of the\n    // direction ray at any given time, it makes sense to only choose\n    // two.\n    //\n    // Declare two normals. Any side by side ones will do.\n    vec2 n1 = vec2(-1, 0), n2 = vec2(0, -1); // Right and top edges.\n    \n    // If the cell wall is behind the ray (or the ray is facing the opposing cell\n    // wall, if you prefer), use the normal index from the back cell wall. This \n    // trick is possible because of the rectangle symmetry. As an aside, for \n    // anyone who doesn't know, dotting the direction ray with the face normal \n    // is something you do in software engines for back face culling.\n    float d1 = dot(rd.xz, n1), d2 = dot(rd.xz, n2);\n    \n    n1 = d1<0.? -n1 : n1;\n    n2 = d2<0.? -n2 : n2;\n    #endif\n    \n    \n    // Initiate the ray position at the ray origin.\n    vec3 pos = ro;\n    \n    // Obtain the coordinates of the cell that the current ray position \n    // is contained in -- I've arranged for the cell coordinates to \n    // represent the cell center to make things easier.\n    vec2 ip = gridID(pos.xz);\n    \n    float t1 = 1e8, t2 = 1e8, t3 = 1e8, tT = 1e8;\n    \n    int hit = 0;\n    \n    \n    // Iterate through 24 cells -- Obviously, if the cells were smaller,\n    // you'd need more to cover the distance.\n    for(int i = 0; i<40; i++){ \n\n         \n        // Height. \n        float ma = h(ip*s);\n        \n         \n        // At this point, we haven't advanced the ray to the back of the cell boundary,\n        // so we're at one of the front cell face positions. Therefore, check to see if \n        // we're under the pylon height. If so, we've hit a face, so mark the face as hit, \n        // then break.\n        if(pos.y<ma){\n            // Hit a side.\n            hit = 1;\n            break; \n        \n        } \n        \n        // Ray intersection from the currect cell position to each of the \n        // visible cell walls. Normals face inward.\n        // You pass in the current position, the unit direction ray, a known \n        // point on the cell wall (any will do) and the cell wall's normal.\n       \n        #ifdef HEXAGON\n        t1 = rayLine(pos.xz, rd.xz, (ip + i1*.5)*s, -n1);\n        t2 = rayLine(pos.xz, rd.xz, (ip + i2*.5)*s, -n2);\n        t3 = rayLine(pos.xz, rd.xz, (ip + i3*.5)*s, -n3);\n        #else\n        t1 = rayLine(pos.xz, rd.xz, (ip + n1*.5)*s, -n1);\n        t2 = rayLine(pos.xz, rd.xz, (ip + n2*.5)*s, -n2);\n        #endif\n        \n        // Determine the closest edge then record the closest distance and\n        // asign its normal index.\n        #ifdef HEXAGON\n        vec3 tn = t1<t2 && t1<t3? vec3(t1, i1) : t2<t3? vec3(t2, i2) : vec3(t3, i3);\n        #else\n        vec3 tn = t1<t2? vec3(t1, n1) : vec3(t2, n2);\n        #endif\n        \n        // Top face distance.\n        tT = (ma - pos.y)/rd.y;\n        tT = tT<0. ? 1e8 : tT;\n        \n        \n        // We've now advanced to one of the back faces of the cell. Check to see whether\n        // we're still under the pylon height, and if so, we've hit the top face --  \n        // I always have to think about this, but the logic is that we haven't hit a front\n        // cell face and we're still under the height, so we've hit the top. Anyway, mark \n        // the top face as hit, advance the distance in the Y direction to the top face, \n        // then break.\n        if(tT<tn.x){\n            \n            //dist += tT;\n            pos += rd*tT; \n            hit = 2;\n            break;\n             \n        }      \n         \n    \n        // If this cell's ID matches the ID of the backgound cell, \n        // flag it as hit in order to color it, or whatever.\n        //if(length(cell - ip)<.001){ hit = 1; break; }\n        \n        // Advance the cell index position by the indices of the \n        // cell wall normal that you hit. \n        ip += tn.yz;\n        // Advance the ray position by the distance to the next cell wall.\n        pos += rd*tn.x;\n    \n    }\n    \n    #ifdef HEXAGON\n    float fID = tT<t1 && tT<t2 && tT<t3? 0. : t1<t2 && t1<t3? 1. : t2<t3? 2. : 3.;\n    if(fID == 1.){ fID = d1<0.? -fID : fID; }\n    else if(fID == 2.){ fID = d2<0.? -fID : fID; }\n    else if(fID == 3.){ fID = d3<0.? -fID : fID; }\n    #else\n    float fID = tT<t1 && tT<t2? 0. : t1<t2? 1. : 2.;\n    if(fID == 1.){ fID = d1<0.? -fID : fID; }\n    else if(fID == 2.){ fID = d2<0.? -fID : fID; }\n    #endif\n    \n    res.x = length(pos - ro);\n    if(hit == 0) res.x = FAR;\n    \n    return vec4(res.x, fID, ip);\n    \n}\n\n// Standard normal function.\nvec3 nr(float fID, vec3 rd) {\n\n    #ifdef HEXAGON\n    vec3 n1 = -vec3(.5, 0, sqrt3/2.); // Right forward.\n    vec3 n2 = -vec3(1, 0, 0); // Right face normal.\n    vec3 n3 = -vec3(-.5, 0, sqrt3/2.); // Left\n    vec3 n = fID == 0.? vec3(0, 1, 0) : abs(fID) == 1.? n1 : abs(fID) == 2.? n2 : n3;\n    if(fID<-.001) n *= -1.;\n    #else\n    vec3 n = fID == 0.? vec3(0, 1, 0) : abs(fID) == 1.? vec3(1, 0, 0) : vec3(0, 0, 1);\n    n *= fID<-.001? -1. : 1.;\n    #endif\n    \n    \n\treturn n;\n}\n\n// mat3 rotation... I did this in a hurry, but I think it's right. :)\n// I have a much better one than this somewhere. \nmat3 rot(vec3 ang){\n    \n    vec3 c = cos(ang), s = sin(ang);\n\n    return mat3(c.x*c.z - s.x*s.y*s.z, -s.x*c.y, -c.x*s.z - s.x*s.y*c.z,\n                c.x*s.y*s.z + s.x*c.z, c.x*c.y, c.x*s.y*c.z - s.x*s.z,\n                c.y*s.z, -s.y, c.y*c.z);    \n}\n\nvoid mainImage(out vec4 fragColor, vec2 fragCoord){\n\n\n\n    #if BUFF_ACCUM == 2\n    // Initial hit point.\n    vec3 resPos = vec3(0);\n    #endif\n    // Overall distance.\n    float resT = 0.;\n\n    // Screen pixel coordinates.\n    vec2 uv0 = (fragCoord - iResolution.xy*.5)/iResolution.y;\n    \n\n    // Initializing the seed value. It needs to be different every frame.\n    seed = uv0 + vec2(fract(iTime/113.671)*.123, fract(iTime/57.913)*.14527);\n    \n    // Ray origin.\n    vec3 ro = vec3(-s.x/2. + iTime*.25, 8., iTime*.25); \n    // Setting the camera to the ray origin. The ray origin vector will change\n    // from bounce to bounce, so we'll need a record of the initial camera position.\n    vec3 cam = ro;\n    \n    \n    // Using the above to produce the unit ray-direction vector.\n    float FOV = 1.; // FOV - Field of view.\n    \n    // Lazy identity camera -- No to and from. I might update it later.\n    mat3 mCam = mat3(vec3(1, 0, 0), vec3(0, 1, 0), vec3(0, 0, 1));\n\n \n    mCam *= rot(vec3(0, 0, cos(iTime/8.*.25)/4. + .35)); // Camera yaw.\n    mCam *= rot(vec3(-sin(iTime/4.*.25)/8., 0, 0)); // Camera roll.\n    mCam *= rot(vec3(0, 1, 0)); // Y axis tilt, or pitch.\n    \n    \n    // Artistic black movie strips. 10% faster \"1337\" democoder move. :D\n    if(abs(uv0.y)>.45) { \n        ivec2 q = ivec2(fragCoord);\n        vec4 c = vec4(0, 0, 0, 1); \n    \tif(q.y == 0 && q.x<3){    \n            // Camera matrix in lower left three pixels, for next frame.\n            if(q.x == 0) c = vec4(mCam[0], -dot(mCam[0], cam));\n            else if(q.x == 1) c = vec4( mCam[1], -dot(mCam[1], cam));\n            else c = vec4( mCam[2], -dot(mCam[2], cam));\n        } \n        fragColor = c;\n        return; \n    }\n \n\n    \n    // Accumulative color and sample number. 8 is all that my computer can \n    // handle. Some computers would be able to handle more and others less.\n    vec3 atot = vec3(0);\n    const int sampNum = 6;\n    \n    for(int j = min(0, iFrame); j<sampNum; j++){\n    \n\n        vec2 jit = hash22() - .5;\n                        \n        vec2 uv = uv0 + jit/iResolution.y;\n    \n        // Unit direction vector.\n        vec3 rd = mCam*normalize(vec3(uv, 1./FOV)); \n /*       \n        // Depth of field. I hacked this in as an afterthought... It seems\n        // about right, but I'll have to take a closer look later.\n        float fDist = 6.;\n        vec2 jitDOF = hash22()*2. - 1.;\n        vec3 vDOF = mCam*vec3(jitDOF, 0.)*.06;\n        rd = normalize(rd - vDOF/fDist);\n        ro = cam + vDOF;\n */\n        ro = cam;\n        \n        // Accumulative, and thoughput.\n        vec3 acc = vec3(0);\n        \n        vec3 through = vec3(1);\n\n        // First hit distance. It's used for fog, amongst other things.\n        float t0; \n        \n  \n        for(int i = min(0, iFrame); i<3; i++){\n\n            // Raycasting\n            vec4 res = raycast(ro, rd);\n\n            float t = res.x, d;\n            float fID = res.y;\n            vec2 id = res.zw;\n\n            t = min(t, FAR); // Clipping to the far distance, which helps avoid artifacts.\n\n            if(i == 0) t0 = t; // Recording the first hit distance.\n\n\n            // Hit point.\n            vec3 p = ro + rd*t;\n            \n            \n            if(i==0){\n                #if BUFF_ACCUM == 2\n                // Only save the initial hit point and distance. Ignore other bounces.\n                resPos += p/float(sampNum); // Accumulative position.\n                #endif\n                resT += t/float(sampNum); // Accumulative distance.\n            }\n            \n            \n    \n            // If we've hit an object, light it up.\n            if(t<FAR - 1e-6){\n            \n                \n                // Surface normal.\n                vec3 n = nr(fID, rd);\n\n                // Scene object color.\n                \n                 // Local coordinates.\n                vec2 lc = p.xz - id*s;\n                \n                // Texture coordinates, based on a cube mapping routine.\n                #ifdef HEXAGON\n                vec2 rp = lc*rot2(atan(n.x, n.z));\n                vec2 tuv = fID == 0.? p.xz : vec2(rp.x, p.y);\n                vec3 tx = texture(iChannel1, tuv/.8660254/3.).xyz; tx *= tx;\n                #else\n                vec2 tuv = fID == 0.? p.xz : abs(fID) == 1.? p.zy : p.xy;\n                vec3 tx = texture(iChannel1, tuv/3.).xyz; tx *= tx;\n                #endif\n\n                vec3 oCol = .125 + tx*2.5;\n            \n\n                // Edging routine.\n                float h0 = h(id*s); // Square prism height.\n\n                float minEdge = min(s.x, s.y)/4.;\n                float edge = s.y/4.;\n                float edge2 = s.x/4.;\n\n               \n         \n                // Edge construction.\n              \n                // Lines eminating from the center to the vertices.\n                #ifdef HEXAGON\n                const float aNum = 6.;\n                vec2 z = rot2(3.14159/aNum)*lc; \n                float a = mod(atan(z.x, z.y), 6.2831)/6.2831;\n                a = (floor(a*aNum) + .5)/aNum;\n                z *= rot2(a*6.2831);\n                // Face edges.\n                vec2 ap = abs(lc);\n                float fEdge = max(ap.y*.8660254 + ap.x*.5, ap.x) - s.x/2.;\n                fEdge = abs(fEdge);\n                fEdge = max(fEdge, -(p.y - h0)) - .01;\n                float sEdge = max(abs(z.x) - .01, (p.y - (h0 - .01)));\n                #else\n                // Domain.\n                vec2 ap = abs(lc) - s/2.;\n                // Face edges.\n                float fEdge = max(ap.x, ap.y);\n                fEdge = abs(fEdge);\n                fEdge = max(fEdge, -(p.y - h0)) - .01;\n                // Side edges.\n                float sEdge = min(ap.x, ap.y);\n                sEdge = max(-sEdge, (p.y - h0)) - .01;\n                #endif\n           \n                // Smoothing facor... Not even sure if it's needed in a multisample\n                // example, but it's here anyway.\n                //float sf = .004;//*(1. + res.x*res.x*.1);\n                \n                // Combining the side and face edges, then smoothstepping.\n                fEdge = min(fEdge, sEdge);\n                \n                // Apply the edges.\n                oCol = mix(oCol, vec3(0), (1. - step(0., fEdge))*.7);\n\n\n\n                // Surface roughness. Larger values are duller in appearance, and lower\n                // values are more relective.\n                float rough = .9;\n\n                // Substance emissive color. Initialized to zero.\n                vec3 emissive = vec3(0);\n                \n                // If we hit square prism strip, color random windows and set their emission color. \n                if(hash21(id + .103)<.1){\n                \n                    // Random window color.\n                    vec3 eCol = .5 + .45*cos(6.2831*hash21(id +.17)/6. + vec3(0, 1.4, 2));\n\n                    //eCol = mix(eCol, eCol.xzy, clamp((h0 - p.y + .5)/4., 0., 1.));\n                    \n                    eCol *= sqrt(eCol);\n                    \n                    // Random emissive color.\n                    emissive = oCol*eCol; // Warm hues.\n                    if(hash21(id +.027)<.25) emissive = oCol*eCol.zyx; // Random cool hues.\n                    // Applying some random green.\n                    //emissive = mix(emissive, emissive.xzy, floor(hash21(id +.42)*4.999)/4.*.35);\n                    // Pink. Too much.\n                    //if(hash21(id +.33)<.2) emissive = mix(emissive, emissive.xzy, .5);\n                    \n\n                    // Randomly turn lights on and off for some visual interest.\n                    float blink = smoothstep(.2, .3, sin(6.2831*hash21(id + .09) + iTime/1.)*.5 + .5);\n                    emissive *= mix(1., 0., blink);\n                   \n                    // Ramp up the emissive power.\n                    emissive *= 16.; \n                    \n                    // Make the glowing pylons less rough, and randomize a bit.\n                    rough = mix(.5, rough, blink);//hash21(id + .21)*.5 + .25;\n                }\n                else {\n                    // Subtly Color the other pylons.\n                    oCol *= (1. + .25*cos(hash21(id + .06)*6.2831/4. + vec3(0, 1, 2)));\n    \n                }\n                \n                //rough *= hash21(floor(tuv*8.) + .21)*.7 + .3;\n\n \n                // Tapering emission into the distance.\n                //emissive = mix(emissive, vec4(0), smoothstep(.25, .99, t0/FAR));\n\n                // If an emissive sustance has been hit, use it to light the surface.\n                acc += emissive*through;\n                through *= oCol; // Integrate colors from previous surfaces. \n\n                \n                vec3 ref = reflect(rd, n); // Purely reflected vector.\n                vec3 rrd = cosDir(0., n); // Random half hemisphere vector.\n\n                // Mimicking surface inconsistancies with fuzzy reflections.\n                // Rougher surfaces have a greater chance of random reflecting at any direction and\n                // smoother surfaces are more likely to purely reflect.\n                float rChance = step(0., rough - hash21(uv + vec2(i*277, j*113) + fract(iTime*.513 + .137)));\n                rd = (mix(ref, rrd, rChance));\n                // Other variations. Not physically correct, but they have their purposes.\n                //float rChance = rough*hash21(uv + vec2(i*277, j*113) + fract(iTime*.97 + .137));\n                //rd = normalize(ref + rrd*rChance);\n                //rd = normalize(mix(ref, rrd, rough));\n                //rd = normalize(ref + normalize(rnd23() - .5)*rChance);  \n                //rd = normalize(ref + rrd*rough);\n\n                // Bump the ray off of the hit surface to avoid self collision.\n                ro = p + n*.001;\n\n            }\n            else { \n                // If the scene hasn't been hit, add a touch of atmospheric haze, then quit.\n                vec3 aCol = vec3(.03, .02, .04)*2.;\n                acc += aCol*through; \n                break;\n            }\n\n    \n        }\n       \n        // Very simple sky fog, or whatever. Not really the way you apply atmosphere in a \n        // path tracer, but way, way cheaper. :)\n        // vec3 sky = mix(vec3(1, .7, .5), vec3(.4, .6, 1), uv0.y*2.5 - .15)*1.5;//vec4(.6, .75, 1, 0)/.6\n        //acc = mix(acc, sky/32., smoothstep(.35, .99, t0/FAR));\n        \n        \n        // Add this sample to the running total.\n        atot += acc;\n        \n    }\n    \n    vec3 col = atot/float(sampNum);\n    \n    \n    // Toning down the high frequency values. A simple Reinhard toner would \n    // get the job done, but I've dropped in a heavily modified and trimmed \n    // down Uncharted 2 tone mapping formula.\n    // mapping function.\n    col = uTone(col.xyzx).xyz; \n    \n     \n    \n    \n    // This is IQ's temporal reprojection code: It's well written and\n    // it makes sense. I wrote some 2D reprojection code and was not\n    // looking forward to writing the 3D version, and then this \n    // suddenly appeared on Shadertoy. If you're interested in rigid \n    // realtime path traced scenes with slowly moving cameras, this is \n    // much appreciated. :)\n    //\n    #if BUFF_ACCUM == 2\n    //-----------------------------------------------\n\t// Reproject to previous frame and pull history.\n    //-----------------------------------------------\n    \n    float kFocLen = 1./FOV;\n    vec3 pos = resPos;\n    ivec2 q = ivec2(fragCoord);\n    col = clamp(col, 0., 1.);\n\n    // Fetch previous camera matrix from the bottom left three pixels.\n    mat3x4 oldCam = mat3x4(texelFetch(iChannel3, ivec2(0, 0), 0),\n                           texelFetch(iChannel3, ivec2(1, 0), 0),\n                           texelFetch(iChannel3, ivec2(2, 0), 0));\n    // World space point.\n    vec4 wpos = vec4(pos, 1.);\n    // Convert to camera space (note inverse multiply).\n    vec3 cpos = wpos*oldCam;\n    // Convert to NDC space (project).\n    vec2 npos = (kFocLen*2.)*cpos.xy/cpos.z;//*iRes/iResolution.y;\n    // Convert to screen space.\n    vec2 spos = .5 + .5*npos*vec2(iResolution.y/iResolution.x, 1);\n\t// Convert to raster space.\n    vec2 rpos = spos*iResolution.xy;\n\n    // Read color+depth from this point's previous screen location.\n    vec4 ocolt = textureLod( iChannel3, spos, 0.);\n    // If we consider the data contains the history for this point.\n    if(iFrame>0 && resT<FAR && (rpos.y>1.5 ||rpos.x>3.5)){\n    \n        // Blend with history (it's an IIR low pas filter really).\n        col = mix( ocolt.xyz, col, 1./8.);\n    }\n    \n    // Color and depth.\n    fragColor = vec4(col, resT);\n    \n    // Output.\n\tif(q.y == 0 && q.x<3){\n    \n    \t// Camera matrix in lower left three pixels, for next frame.\n        if(q.x == 0) fragColor = vec4(mCam[0], -dot(mCam[0], cam));\n        else if(q.x == 1) fragColor = vec4( mCam[1], -dot(mCam[1], cam));\n        else fragColor = vec4( mCam[2], -dot(mCam[2], cam));\n    } \n    #elif BUFF_ACCUM == 1\n    // Mix the previous frames in with no camera reprojection.\n    // It's OK, but full temporal blur will be experienced.\n    vec4 preCol = texelFetch(iChannel3, ivec2(fragCoord), 0);\n    float blend = (iFrame < 2) ? 1. : 1./4.; \n    fragColor = mix(preCol, vec4(clamp(col, 0., 1.), resT), blend);\n    #else\n    // No reprojection or temporal blur, for comparisson.\n    fragColor = vec4(max(col, 0.), resT);\n    #endif\n    \n\n    \n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "texture",
                        "id": 3,
                        "published": 1,
                        "sampler": {
                            "filter": "mipmap",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "repeat"
                        },
                        "src": "/media/a/95b90082f799f48677b4f206d856ad572f1d178c676269eac6347631d4447258.jpg"
                    },
                    {
                        "channel": 0,
                        "ctype": "texture",
                        "id": 16,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "repeat"
                        },
                        "src": "/media/a/3083c722c0c738cad0f468383167a0d246f91af2bfa373e9c5c094fb8c8413e0.png"
                    },
                    {
                        "channel": 3,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "// I didn't feel like writing my own denoising routine, so did a quick search\n// and came up with BrutPitt's exaple, here: https://www.shadertoy.com/view/3dd3Wr\n//\n// I chose it because it works reasonably well and is simple. I made some minor, \n// but necessary changes. There really is no substitute for increased sample count, \n// but denoisers have their place. At some stage, I'll get in amongst it and write \n// my own... or I'll cross my fingers and hope that someone else writes a really \n// good one.\n\n//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n//  Copyright (c) 2018-2019 Michele Morrone\n//  All rights reserved.\n//\n//  https://michelemorrone.eu - https://BrutPitt.com\n//\n//  me@michelemorrone.eu - brutpitt@gmail.com\n//  twitter: @BrutPitt - github: BrutPitt\n//  \n//  https://github.com/BrutPitt/glslSmartDeNoise/\n//\n//  This software is distributed under the terms of the BSD 2-Clause license\n//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\n#define INV_SQRT_OF_2PI .3989422804  // 1/SQRT_OF_2PI\n#define INV_PI .31830988618379 // 1/PI\n\n//  smartDeNoise - parameters\n//~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n//\n//  sampler2D tex     - sampler image / texture\n//  vec2 uv           - actual fragment coord\n//  float sigma  >  0 - sigma Standard Deviation\n//  float kSigma >= 0 - sigma coefficient \n//  kSigma * sigma  -->  radius of the circular kernel\n//  float threshold   - edge sharpening threshold \n\nvec4 smartDeNoise(sampler2D tex, vec2 uv, float sigma, float kSigma, float threshold){\n \n    float radius = round(kSigma*sigma);\n    float radQ = radius*radius;\n    \n    float invSigmaQx2 = .5/(sigma*sigma);     // 1/(sigma^2*2)\n    float invSigmaQx2PI = INV_PI*invSigmaQx2; // 1/(2*PI*sigma^2)\n    \n    float invThresholdSqx2 = .5/(threshold*threshold);     // 1./(sigma^2*2x)\n    float invThresholdSqrt2PI = INV_SQRT_OF_2PI/threshold;   // 1/(sqrt(2*PI)*sigma)\n    \n    vec4 centrPx = texture(tex, uv);\n    \n    float zBuff = 0.;\n    vec3 aBuff = vec3(0);\n    //vec2 size = vec2(textureSize(tex, 0));\n    \n    for(float x = -radius; x <= radius; x++) {\n        float pt = sqrt(radQ - x*x);  // pt = yRadius: have circular trend\n        for(float y = -pt; y <= pt; y++) {\n            \n            vec2 d = vec2(x,y);\n\n            float blurFactor = exp(-dot(d, d)*invSigmaQx2)*invSigmaQx2PI; \n            \n            vec3 walkPx = texture(tex, uv + d/iResolution.xy).xyz;\n\n            vec3 dC = walkPx - centrPx.xyz;\n            float deltaFactor = exp(-dot(dC, dC)*invThresholdSqx2)*invThresholdSqrt2PI*blurFactor;\n                                 \n            zBuff += deltaFactor;\n            aBuff += deltaFactor*walkPx;\n        }\n    }\n    \n    return vec4(aBuff/zBuff, centrPx.w);\n}\n \n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord){\n\n\n    // Coordinates.\n    vec2 uv = fragCoord/iResolution.xy;\n     \n    // Retrieving the stored color.\n    vec4 col = smartDeNoise(iChannel0, uv, 4., 1., .08);\n  \n    // Rough gamma correction and screen presentation.\n    fragColor = col;\n    \n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer B",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 258
                    }
                ],
                "type": "buffer"
            }
        ],
        "ver": "0.1"
    }
}