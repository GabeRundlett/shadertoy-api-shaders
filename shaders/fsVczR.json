{
    "Shader": {
        "info": {
            "date": "1654319734",
            "description": "A method to compute world space normals from a depth buffer that avoids the usual discontinuities at the edges of the objects, described by Yuwen Wu here [url]https://atyuwen.github.io/posts/normal-reconstruction[/url]. Left: naive. Right: improve",
            "flags": 32,
            "hasliked": 0,
            "id": "fsVczR",
            "likes": 75,
            "name": "Normals from Depth",
            "published": 3,
            "tags": [
                "3d",
                "normals",
                "depth"
            ],
            "usePreview": 0,
            "username": "iq",
            "viewed": 5598
        },
        "renderpass": [
            {
                "code": "// When computing world space normals from a camera space depth buffer\n// the naive way, artifact appear at the edges of the objects. This\n// shaders shows one way to reduce these artifacts, described by Yuwen Wu\n// here: https://atyuwen.github.io/posts/normal-reconstruction\n//\n// On the left you see the naive method to compute normals, which produces\n// wrong results. On the right side you see the improved method which is\n// very close to the ground truth, as long as the surface details are larger\n// than 2 pixels. Both the naive and improved method are compared to the\n// ground truth normals every second.\n//\n// Like the naive method, the improved one computes world space normals\n// by considering the world position derivatives with respect to the\n// pixel's X and Y coordinates.\n//\n// However, instead of using central differences blindly, it picks either\n// the left or right neighbor based on which one is estimated to belong to\n// the same surface as the pixel for which we are currently computing a\n// normal. This is done by examining a second pixel to the right and left\n// and doing a comparison between the current pixel's depth and that we'd\n// expect to have at the current pixel if the neighbors were forming a\n// plane. Because such interpolation is linear, we do it in 1/z rather\n// than z space. So,\n//\n// 1/(1/a+1/a-1/b) rather than a+a-b, which leads to ab/(2b-a)\n//\n// The side with lowest discontinuity is picked, to avoid edge artifacts\n// as much as poosible.\n\n// recoved world space from depth\nvec3 getPos( in ivec2 fragCoord, in float depth );\n\n// computes the normal at pixel \"p\" based on the deph buffer \"depth\"\nvec3 computeNormalImproved( const sampler2D depth, in ivec2 p )\n{\n    float c0 = texelFetch(depth,p           ,0).w;\n    float l2 = texelFetch(depth,p-ivec2(2,0),0).w;\n    float l1 = texelFetch(depth,p-ivec2(1,0),0).w;\n    float r1 = texelFetch(depth,p+ivec2(1,0),0).w;\n    float r2 = texelFetch(depth,p+ivec2(2,0),0).w;\n    float b2 = texelFetch(depth,p-ivec2(0,2),0).w;\n    float b1 = texelFetch(depth,p-ivec2(0,1),0).w;\n    float t1 = texelFetch(depth,p+ivec2(0,1),0).w;\n    float t2 = texelFetch(depth,p+ivec2(0,2),0).w;\n    \n    float dl = abs(l1*l2/(2.0*l2-l1)-c0);\n    float dr = abs(r1*r2/(2.0*r2-r1)-c0);\n    float db = abs(b1*b2/(2.0*b2-b1)-c0);\n    float dt = abs(t1*t2/(2.0*t2-t1)-c0);\n    \n    vec3 ce = getPos(p,c0);\n\n    vec3 dpdx = (dl<dr) ?  ce-getPos(p-ivec2(1,0),l1) : \n                          -ce+getPos(p+ivec2(1,0),r1) ;\n    vec3 dpdy = (db<dt) ?  ce-getPos(p-ivec2(0,1),b1) : \n                          -ce+getPos(p+ivec2(0,1),t1) ;\n\n    return normalize(cross(dpdx,dpdy));\n}\n\n// naive way of computing the normal\nvec3 computeNormalNaive( const sampler2D depth, in ivec2 p )\n{\n    vec3 l1 = getPos(p-ivec2(1,0),texelFetch(depth,p-ivec2(1,0),0).w);\n    vec3 r1 = getPos(p+ivec2(1,0),texelFetch(depth,p+ivec2(1,0),0).w);\n    vec3 t1 = getPos(p+ivec2(0,1),texelFetch(depth,p+ivec2(0,1),0).w);\n    vec3 b1 = getPos(p-ivec2(0,1),texelFetch(depth,p-ivec2(0,1),0).w);\n    vec3 dpdx = r1-l1;\n    vec3 dpdy = t1-b1;\n    return normalize(cross(dpdx,dpdy));\n}\n\n// compute the world space position of a pixel with coordinates\n// fragCoord and distance \"depth\" to camera. This will need to\n// change depending on wether your depth buffer stores \"depth\"\n// \"z\", \"reverse z\", etc\nvec3 getPos( in ivec2 fragCoord, in float depth )\n{\n    vec2 p = (2.0*vec2(fragCoord)-iResolution.xy)/iResolution.y;\n    vec3 ro, rd;\n    camera( ro, rd, iTime, p );\n    return depth*rd;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    ivec2 p = ivec2(fragCoord);\n \n    //----------\n    // compute\n    //----------\n\n    // ground truth normal\n    vec3 norT = texelFetch( iChannel0, p, 0 ).xyz;\n \n    // naive way of computing the normal\n    vec3 norN = computeNormalNaive( iChannel0, p );\n    \n    // improved normal computation\n    vec3 norI = computeNormalImproved( iChannel0, p );\n\n    //----------\n    // display\n    //----------\n\n    // left: naive, right: improved\n    float x = fragCoord.x/iResolution.x;\n    vec3 nor = (x<0.5) ? norN : norI;\n    \n    // compare to true normal\n    if( sin(0.5*6.283185*iTime)<0.0 ) nor = norT;\n\n    // color : switch normals and lighting\n    vec3 col = nor;\n    if( sin(6.283185*iTime/16.0)<0.0 ) col = vec3(0.1,0.15,0.2)*nor.y + vec3(1.0,0.9,0.85)*vec3(nor.x*0.8+nor.y*0.5+nor.z*0.6);\n    \n    // depth darkening to hide geometry aliasing\n    float t = texelFetch( iChannel0, p, 0 ).w;\n    col *= exp2(-0.12*t*t);\n    \n    // separation bar\n    col = mix( vec3(1.0), col, smoothstep( 0.002, 0.003, abs(x-0.5) ) );\n    \n    fragColor = vec4(col,1.0);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "//\n// Simplified geometry from https://www.shadertoy.com/view/lsf3zr\n//\n\n\n// https://iquilezles.org/articles/distfunctions\nfloat sdBox( in vec3 p, in vec3 b ) \n{\n    vec3 q = abs(p) - b;\n    return min(max(q.x,max(q.y,q.z)),0.0) + length(max(q,0.0));\n}\n\n// https://iquilezles.org/articles/smin\nfloat smin( float a, float b, float k )\n{\n    float h = max(k-abs(a-b),0.0);\n    return min(a, b) - h*h*0.25/k;\n}\n\n//------------------------------------------\n\nfloat column( in float x, in float y, in float z )\n{\n    float y2=y-0.25;\n    float y4=y-1.0;\n\n    const float sqh = sqrt(0.5);\n    float nx = max(abs(x),abs(z));\n    float nz = min(abs(x),abs(z));\t\n\n    float dsp = abs(min(cos(1.125*6.283185*x/0.085), \n                        cos(1.125*6.283185*z/0.085)));\n    dsp *= 1.0-smoothstep(0.8,0.9,abs(x/0.085)*abs(z/0.085));\n    \n    float di1 = sdBox( vec3(x,y,z),   vec3(0.085+dsp*0.0075,1.0,0.085+dsp*0.0075));\n    float di2 = sdBox( vec3(x,y,z),   vec3(0.12,0.29,0.12) );\n    float di3 = sdBox( vec3(x,y4,z),  vec3(0.14,0.02,0.14) );\n    float di4 = sdBox( vec3(nx,y,nz), vec3(0.14,0.3,0.05) );\n    float di5 = sdBox( vec3(nx,(y2+nz)*sqh,(nz-y2)*sqh), vec3(0.12, 0.16*sqh, 0.16*sqh));\n    float di6 = sdBox( vec3(nx,(y2+nz)*sqh,(nz-y2)*sqh), vec3(0.14, 0.10*sqh, 0.10*sqh));\n\n    return min(min(min(di1,di2),\n                   min(di3,di4)),\n                   min(di5,di6));\n}\n\nfloat wave( in float x, in float y )\n{\n    return sin(x)*sin(y);\n}\n\nfloat map( vec3 pos )\n{\n    // floor\n    vec2 id = floor((pos.xz+0.1)/0.2 );\n    float h = 0.012 + 0.008*sin(id.x*2313.12+id.y*3231.219);\n    vec3 ros = vec3( mod(pos.x+0.1,0.2)-0.1, pos.y, mod(pos.z+0.1,0.2)-0.1 );\n    float res = sdBox( ros, vec3(0.096,h,0.096) );\n\n    // ceilin\n\tfloat x = fract( pos.x+128.0 ) - 0.5;\n\tfloat z = fract( pos.z+128.0 ) - 0.5;\n    float y = (1.0 - pos.y)*0.6;\n    float dis = 0.4 - smin(sqrt(y*y+x*x),sqrt(y*y+z*z),0.01);\n    float dsp = abs(sin(31.416*pos.y)*sin(31.416*pos.x)*sin(31.416*pos.z));\n    dis -= 0.02*dsp;\n\tdis = max( dis, y );\n    res = min( res, dis );\n\n    // columns\n\tvec2 fc = fract( pos.xz+128.5 ) - 0.5;\n\tfloat dis2 = column( fc.x, pos.y, fc.y );\n    res = min( res, dis2 );\n    \n    return res;\n}\n\nfloat raycast( in vec3 ro, in vec3 rd, in float precis, in float maxd )\n{\n    float t = 0.001;\n    for( int i=0; i<128; i++ )\n    {\n\t    float d = map( ro+rd*t );\n        if( abs(d)<(precis*t)||t>maxd ) break;\n        t += d;\n    }\n    if( t>maxd ) t=-1.0;\n    return t;\n}\n\n// https://iquilezles.org/articles/normalsSDF\nvec3 calcNormal( in vec3 pos )\n{\n\tconst vec2 eps = vec2( 0.0002, 0.0 );\n\treturn normalize(vec3(\n\t    map(pos+eps.xyy) - map(pos-eps.xyy),\n\t    map(pos+eps.yxy) - map(pos-eps.yxy),\n\t    map(pos+eps.yyx) - map(pos-eps.yyx) ));\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    vec2 p = (2.0*fragCoord-iResolution.xy)/iResolution.y;\n\n    vec3 ro, rd;\n    camera( ro, rd, iTime, p );\n\n    float t= raycast(ro,rd,0.00001,100.0);\n    if( t>0.0 )\n    {\n        vec3 pos = ro + t*rd;\n        vec3 nor = calcNormal( pos );\n        fragColor = vec4(nor,t);\n    }\n    else\n    {\n        fragColor = vec4( 0.0, 0.0, 0.0, 1e10 );\n    }\n}",
                "description": "",
                "inputs": [],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "void camera( out vec3 ro, out vec3 rd, in float time, in vec2 p)\n{\n    // screen split\n    p.x -= sign(p.x)*1.77777*0.5;\n\n    // camera position and target\n    ro = vec3(0.5, 0.3, 0.5 );\n    vec3 ta = ro + vec3( -1.0, 0.0, -1.0 );\n    \n    // contruct ray\n    vec3 cw = normalize( ta-ro );\n    vec3 cp = vec3( 0.0, 1.0, 0.0 );\n    vec3 cu = normalize( cross(cw,cp) );\n    vec3 cv = normalize( cross(cu,cw) );\n    rd = normalize( p.x*cu + p.y*cv + 1.8*cw );\n}",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            }
        ],
        "ver": "0.1"
    }
}