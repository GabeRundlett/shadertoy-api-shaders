{
    "Shader": {
        "info": {
            "date": "1597279816",
            "description": "Real-time global illumination via Monte Carlo path tracing with Spatiotemporal Variance-Guided Filtering.\n\nUse the mouse and WASD (or arrow keys) to move the camera.\n\nWork in progress.\n\n\"Raytracing is not slow, computers are.\" ~ Kajiya\n",
            "flags": 48,
            "hasliked": 0,
            "id": "wtXfRX",
            "likes": 52,
            "name": "Real-time global illumination",
            "published": 3,
            "tags": [
                "gi",
                "pathtracing"
            ],
            "usePreview": 0,
            "username": "nlight",
            "viewed": 2973
        },
        "renderpass": [
            {
                "code": "// Copyright 2020 Alexander Dzhoganov\n//\n// MIT License\n//\n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n//\n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n//\n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n// SOFTWARE.\n\n/*\n *\n * Real-time global illumination via Monte Carlo path tracing\n * with Spatiotemporal Variance-Guided Filtering.\n *\n * >>>\n * >>> Use the mouse and WASD (or arrow keys) to move the camera.\n * >>>\n *\n * You can increase or decrease the quality (and performance) by changing\n * NUM_SAMPLES in Buffer A. HISTORY_BLEND_FACTOR in Buffer C controls the\n * trade-off between ghosting and noise.\n *\n * The code is pretty well commented and hopefully easy to read.\n * Various quality trade-offs were made due to the very thin gbuffer layout and\n * not enough buffers.\n *\n * > Spatiotemporal Variance-Guided Filtering:\n *\n * Dundr 2018, \"Progressive Spatiotemporal Variance-Guided Filtering\"\n * https://pdfs.semanticscholar.org/a81a/4eed7f303f7e7f3ca1914ccab66351ce662b.pdf\n *\n * NVIDIA 2017, \"Spatiotemporal Variance-Guided Filtering: Real-Time\n * Reconstruction for Path-Traced Global Illumination\"\n * https://cg.ivd.kit.edu/publications/2017/svgf/svgf_preprint.pdf\n *\n * Dammertz, Sewtz, Hanika, Lensch 2010, \"Edge-Avoiding Ã€-Trous Wavelet\n * Transform for fast Global Illumination Filtering\"\n * https://jo.dreggn.org/home/2010_atrous.pdf\n *\n * > Pseudorandom number generation for Monte Carlo integration:\n *\n * GPU Gems 3, \"Efficient Random Number Generation and Application Using CUDA\"\n * https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-37-efficient-random-number-generation-and-application\n *\n * > Branchless construction of an orthonormal basis:\n *\n * Pixar 2017, \"Building an Orthonormal Basis, Revisited\"\n * https://graphics.pixar.com/library/OrthonormalB/paper.pdf\n *\n */\n\n// different debug view modes\n#define DEBUG_MODE 0\n// 0 - debug off\n// 1 - albedo\n// 2 - normals\n// 3 - depth\n// 4 - irradiance\n// 5 - variance\n// 6 - age\n\nbool debugDrawGbuffer(GBuffer gbuf, out vec4 fragColor) {\n  switch (DEBUG_MODE) {\n  case 1: // albedo\n    fragColor = vec4(gbuf.albedo, 1.0);\n    break;\n  case 2: // normals\n    fragColor = vec4(vec3(gbuf.normal * 0.5 + 0.5), 1.0);\n    break;\n  case 3: // depth\n    fragColor = vec4(vec3(gbuf.depth * 0.01), 1.0);\n    break;\n  case 4: // radiance\n    fragColor = vec4(vec3(pow(gbuf.radiance, 1.0 / 2.2)), 1.0);\n    break;\n  case 5: // variance\n    fragColor = vec4(vec3(gbuf.variance), 1.0);\n    break;\n  case 6: // age\n    fragColor = vec4(vec3(gbuf.age), 1.0);\n    break;\n  }\n\n  return DEBUG_MODE != 0;\n}\n\n// Narkowicz 2015, \"ACES Filmic Tone Mapping Curve\"\n// https://knarkowicz.wordpress.com/2016/01/06/aces-filmic-tone-mapping-curve/\nvec3 aces(vec3 x) {\n  const float a = 2.51;\n  const float b = 0.03;\n  const float c = 2.43;\n  const float d = 0.59;\n  const float e = 0.14;\n  return clamp((x * (a * x + b)) / (x * (c * x + d) + e), 0.0, 1.0);\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // fourth filtering pass (step size = 8)\n  GBuffer g = psvgf(iChannel0, ivec2(fragCoord), 8);\n\n  // if any debug mode is active draw it and bail\n  if (debugDrawGbuffer(g, fragColor)) {\n    return;\n  }\n\n  // calculate the final color value of the pixel\n  vec3 color = g.albedo * g.radiance;\n\n  // hack to avoid showing first frame artifacts\n  if (iFrame == 0) {\n    color = vec3(0.0);\n  }\n\n  // tonemapping\n  color = aces(color);\n\n  // gamma correction\n  color = pow(color, vec3(1.0 / 2.2));\n\n  fragColor = vec4(color, 1.0);\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 260,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer03.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "\nconst float MOUSE_SENSITIVITY = 0.15; // mouse sensitivity\nconst uint NUM_SAMPLES = 16u; // number of traced paths per pixel per frame\nconst uint MAX_BOUNCES = 2u;  // max number of bounces per path\n\n// Ray-sphere intersection.\nfloat intersectRaySphere(vec3 ro, vec3 rd, vec3 sp, float rsq) {\n  vec3 n = ro - sp;\n  float a = dot(rd, rd);\n  float b = 2.0 * dot(rd, n);\n  float c = dot(n, n) - rsq;\n  float d = b * b - 4.0 * a * c;\n  return d < 0.0 ? -1.0 : (-b - sqrt(d)) / 2.0 * a;\n}\n\n// Ray-plane intersection.\nfloat intersectRayPlane(vec3 ro, vec3 rd, vec3 n, vec3 p) {\n  const float eps = 0.0001;\n  float denom = dot(rd, n);\n  return abs(denom) < eps ? -1.0 : dot(p - ro, n) / denom;\n}\n\n// Ray-triangle intersection.\n// https://en.wikipedia.org/wiki/M%C3%B6ller%E2%80%93Trumbore_intersection_algorithm\nfloat intersectRayTri(vec3 ro, vec3 rd, vec3 v0, vec3 v1, vec3 v2) {\n  const float eps = 0.0000001;\n\n  vec3 e1 = v1 - v0;\n  vec3 e2 = v2 - v0;\n  vec3 h = cross(rd, e2);\n  float a = dot(e1, h);\n  if (a > -eps && a < eps) {\n    return -1.0;\n  }\n\n  float f = 1.0 / a;\n  vec3 s = ro - v0;\n  float u = f * dot(s, h);\n  if (u < 0.0 || u > 1.0) {\n    return -1.0;\n  }\n\n  vec3 q = cross(s, e1);\n  float v = f * dot(rd, q);\n  if (v < 0.0 || u + v > 1.0) {\n    return -1.0;\n  }\n\n  float t = f * dot(e2, q);\n  if (t < eps) {\n    return -1.0;\n  }\n\n  return t;\n}\n\n// Efficient PRNG.\n// https://developer.nvidia.com/gpugems/gpugems3/part-vi-gpu-computing/chapter-37-efficient-random-number-generation-and-application\n\nuvec4 rngState;\n\nuint tausStep(uint z, uint S1, uint S2, uint S3, uint M) {\n  uint b = ((z << S1) ^ z) >> S2;\n  return ((z & M) << S3) ^ b;\n}\n\nuint lcgStep(uint z, uint A, uint C) { return A * z + C; }\n\n// Returns a random number [0..1].\nfloat random() {\n  const float c = 2.3283064365387e-10;\n  rngState.x = tausStep(rngState.x, 13u, 19u, 12u, 4294967294u);\n  rngState.y = tausStep(rngState.y, 2u, 25u, 4u, 4294967288u);\n  rngState.z = tausStep(rngState.z, 3u, 11u, 17u, 4294967280u);\n  rngState.w = lcgStep(rngState.w, 1664525u, 1013904223u);\n  return saturate(c * float(rngState.x ^ rngState.y ^ rngState.z ^ rngState.w));\n}\n\nfloat hash(vec2 p) {\n  return fract(1e4 * sin(17.0 * p.x + p.y * 0.1) *\n               (0.1 + abs(sin(p.y * 13.0 + p.x))));\n}\n\n// Seeds the random number generator.\nvoid seedRng(vec4 seed) {\n  seed.x = hash(seed.xy);\n  seed.y = hash(seed.yz);\n  seed.z = hash(seed.zw);\n  seed.w = hash(seed.wx);\n\n  rngState = floatBitsToUint(seed);\n  rngState ^= (rngState << 13);\n  rngState ^= (rngState >> 17);\n  rngState ^= (rngState << 5);\n}\n\n// Branchless construction of an orthonormal basis.\n// https://graphics.pixar.com/library/OrthonormalB/paper.pdf\nvoid orthonormalBasis(const vec3 n, out vec3 b1, out vec3 b2) {\n  float s = n.z >= 0.0 ? 1.0 : -1.0;\n  float a = -1.0 / (s + n.z);\n  float b = n.x * n.y * a;\n  b1 = vec3(1.0 + s * n.x * n.x * a, s * b, -s * n.x);\n  b2 = vec3(b, s + n.y * n.y * a, -n.y);\n}\n\n// Returns a random cosine-weighted unit vector on a hemisphere centered around\n// n.\nvec3 unitVectorOnHemisphere(vec3 n) {\n  float r = random();\n  float angle = random() * (2.0 * PI);\n  float sr = sqrt(r);\n  vec2 p = vec2(sr * cos(angle), sr * sin(angle));\n  vec3 ph = vec3(p.xy, sqrt(1.0 - dot(p, p)));\n\n  vec3 b1, b2;\n  orthonormalBasis(n, b1, b2);\n  return b1 * ph.x + b2 * ph.y + n * ph.z;\n}\n\nstruct Sphere {\n  vec3 position;\n  float radius;\n  vec4 color; // rgb - albedo, a - emissive\n};\n\nstruct Tri {\n  vec3 v0;\n  vec3 v1;\n  vec3 v2;\n  vec4 color;\n};\n\nconst vec4 white = vec4(1.0, 1.0, 1.0, 0.0);\nconst vec4 red = vec4(0.9, 0.15, 0.15, 0.0);\nconst vec4 green = vec4(0.15, 0.8, 0.05, 0.0);\nconst vec4 blue = vec4(0.15, 0.3, 0.95, 0.0);\nconst vec4 pink = vec4(0.95, 0.71, 0.75, 0.0);\nconst vec4 orange = vec4(0.95, 0.85, 0.05, 0.0);\n\nconst Tri[] sceneTris = Tri[](\n    Tri(vec3(55.28, 0, 0), vec3(0, 0, 0), vec3(0, 0, 55.92), white),\n    Tri(vec3(55.28, 0, 0), vec3(0., 0, 55.92), vec3(54.96, 0, 55.92), white),\n    Tri(vec3(54.96, 0, 55.92), vec3(0., 0, 55.92), vec3(0, 54.88, 55.92),\n        white),\n    Tri(vec3(54.96, 0, 55.92), vec3(0, 54.88, 55.92), vec3(55.60, 54.88, 55.92),\n        white),\n    Tri(vec3(55.28, 0, 0), vec3(54.96, 0, 55.92), vec3(55.60, 54.88, 55.92),\n        red),\n    Tri(vec3(55.28, 0, 0), vec3(55.60, 54.88, 55.92), vec3(55.60, 54.88, 0),\n        red),\n    Tri(vec3(0, 0, 55.92), vec3(0, 0, 0), vec3(0, 54.88, 0), green),\n    Tri(vec3(0, 0, 55.92), vec3(0, 54.88, 0), vec3(0, 54.88, 55.92), green),\n    Tri(vec3(55.60, 54.88, 0), vec3(55.60, 54.88, 55.92), vec3(0, 54.88, 55.92),\n        white),\n    Tri(vec3(55.60, 54.88, 0), vec3(0, 54.88, 55.92), vec3(0, 54.88, 0),\n        white));\n\nconst Sphere[] sceneSpheres =\n    Sphere[](Sphere(vec3(42.0, 40.0, 30.5), 7.5, // red ball\n                    red),\n             Sphere(vec3(5.6, 15.0, 20.0), 5.5, // orange ball\n                    orange),\n             Sphere(vec3(40.0, 6.0, 35.0), 6.0, // pink ball\n                    pink),\n             Sphere(vec3(12.0, 32.0, 45.0), 5.5, // green ball\n                    green),\n             Sphere(vec3(22.0, 8.0, 25.0), 5.5, // blue ball\n                    blue),\n             Sphere(vec3(35.0, 22.0, 30.0), 8.0, // white ball 1\n                    vec4(1.0, 1.0, 1.0, 3.5)),\n             Sphere(vec3(15.0, 40.0, 50.0), 4.0, // white ball 2\n                    vec4(1.0, 1.0, 1.0, 2.5)));\n\n// Traces a ray (ro, rd) through the scene and returns the hit distance, normal\n// and color.\nfloat traceSceneRay(vec3 ro, vec3 rd, out vec3 normal, out vec4 color) {\n  const vec3 up = vec3(0.0, 1.0, 0.0);\n\n  float minT = 1e10;\n  color = vec4(1.0, 1.0, 1.0, 0.0);\n\n  for (int i = 0; i < sceneSpheres.length(); i++) {\n    Sphere sphere = sceneSpheres[i];\n    vec3 p = sphere.position;\n    float r2 = sphere.radius * sphere.radius;\n    float t = intersectRaySphere(ro, rd, p, r2);\n    if (t > 0.0 && t < minT) {\n      normal = (ro + rd * t) - p;\n      color = sphere.color;\n      minT = t;\n    }\n  }\n\n  for (int i = 0; i < sceneTris.length(); i++) {\n    Tri tri = sceneTris[i];\n    float t = intersectRayTri(ro, rd, tri.v0, tri.v1, tri.v2);\n    if (t > 0.0 && t < minT) {\n      normal = normalize(cross(tri.v1 - tri.v0, tri.v2 - tri.v0));\n      color = tri.color;\n      minT = t;\n    }\n  }\n\n  return minT;\n}\n\n// Traces multiple paths for a primary ray and returns the blended result.\nGBuffer tracePrimaryRay(vec3 ro, vec3 rd) {\n  GBuffer gbuf;\n\n  vec3 normal0;\n  vec4 color;\n  // get the depth, normal and color for this ray\n  float depth = traceSceneRay(ro, rd, normal0, color);\n  normal0 = normalize(normal0);\n\n  float emissive = color.a;\n\n  // fill the gbuffer with the material data\n  gbuf.albedo = color.rgb;\n  gbuf.depth = depth;\n  gbuf.normal = normal0;\n\n  // move the ray to the hit point\n  ro += rd * depth;\n  // slightly displace by the normal to prevent self-intersection\n  ro += normal0 * 0.00001;\n\n  vec3 ro0 = ro;\n\n  // radiance sum for this pixel\n  float radiance = 0.0;\n\n  // radiance squared sum for this pixel\n  // used later on for variance estimation\n  float radiance2 = 0.0;\n\n  for (uint q = UZERO; q < NUM_SAMPLES; q++) {\n    // get a random direction on the hemisphere around the normal\n    ro = ro0;\n    rd = unitVectorOnHemisphere(normal0);\n\n    // radiance sum for the current path\n    float r = 0.0;\n\n    // keep bouncing and gathering light\n    for (uint i = UZERO; i < MAX_BOUNCES; i++) {\n      vec3 normal;\n      depth = traceSceneRay(ro, rd, normal, color);\n      if (depth > 100.0) {\n        break;\n      }\n\n      // gather whatever we hit\n      r += color.a;\n\n      // calculate the ray for the next bounce\n      normal = normalize(normal);\n      ro += rd * depth;\n      ro += normal * 0.00001;\n      rd = unitVectorOnHemisphere(normal);\n    }\n\n    // add to the total radiance\n    radiance += r;\n    radiance2 += r * r;\n  }\n\n  radiance /= float(NUM_SAMPLES);\n  radiance2 /= float(NUM_SAMPLES);\n\n  // variance = sum(x^2) - sum(x)^2\n  gbuf.variance = radiance2 - radiance * radiance;\n  gbuf.radiance = radiance + emissive;\n\n  return gbuf;\n}\n\n#define KEY_A 65\n#define KEY_D 68\n#define KEY_S 83\n#define KEY_W 87\n#define KEY_LEFT 37\n#define KEY_UP 38\n#define KEY_RIGHT 39\n#define KEY_DOWN 40\n\nbool isKeyDown(int key) {\n  return texelFetch(iChannel1, ivec2(key, 0), 0).x != 0.0;\n}\n\n// Updates the camera according to user inputs.\nvoid updateCamera(inout CameraData camera, mat4 cameraMatrix) {\n  if (iFrame == 0) {\n    camera.position = vec3(27.8, 27.3, -100.0);\n    camera.pitchYaw = vec2(0.0, PI);\n    camera.prevMouse = iMouse.xy;\n  }\n\n  vec3 camFwd = (cameraMatrix * vec4(0.0, 0.0, -1.0, 0.0)).xyz;\n  vec3 camRight = (cameraMatrix * vec4(1.0, 0.0, 0.0, 0.0)).xyz;\n  float moveSpeed = 16.0 * iTimeDelta;\n\n  if (isKeyDown(KEY_W) || isKeyDown(KEY_UP)) {\n    camera.position += camFwd * moveSpeed;\n  }\n  if (isKeyDown(KEY_S) || isKeyDown(KEY_DOWN)) {\n    camera.position -= camFwd * moveSpeed;\n  }\n  if (isKeyDown(KEY_A) || isKeyDown(KEY_LEFT)) {\n    camera.position -= camRight * moveSpeed;\n  }\n  if (isKeyDown(KEY_D) || isKeyDown(KEY_RIGHT)) {\n    camera.position += camRight * moveSpeed;\n  }\n\n  vec2 mouseDelta = iMouse.xy - camera.prevMouse;\n  mouseDelta = clamp(mouseDelta, -7.0, 7.0);\n  mouseDelta.x *= -1.0;\n  camera.pitchYaw -= mouseDelta.yx * iTimeDelta * MOUSE_SENSITIVITY;\n  camera.pitchYaw.x = clamp(camera.pitchYaw.x, -PI, PI);\n  camera.prevMouse = iMouse.xy;\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // seed the rng\n  seedRng(vec4(fragCoord.xy, iFrame, iTime));\n\n  // fetch the camera data\n  vec4 cameraDataRaw = texelFetch(iChannel0, ivec2(0, 0), 0);\n  CameraData camera = unpackCameraData(cameraDataRaw);\n  mat4 cameraMatrix = getInvViewMatrix(camera);\n\n  if (uint(fragCoord.x) == 0u && uint(fragCoord.y) == 0u) {\n    // update the camera and store the updated data\n    updateCamera(camera, cameraMatrix);\n    fragColor = packCameraData(camera);\n    return;\n  } else if (uint(fragCoord.x) == 1u && uint(fragCoord.y) == 0u) {\n    // store previous frame's camera (used later on)\n    fragColor = cameraDataRaw;\n    return;\n  }\n\n  // get the camera ray for this pixel\n  vec3 ro = camera.position;\n  vec3 rd = rayDirection(55.0, iResolution.xy, fragCoord);\n  rd = (cameraMatrix * vec4(rd, 0.0)).xyz;\n\n  // trace the ray\n  GBuffer gbuf = tracePrimaryRay(ro, rd);\n\n  // store the result\n  fragColor = packGBuffer(gbuf);\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "keyboard",
                        "id": 33,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/presets/tex00.jpg"
                    },
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "\nconst float HISTORY_BLEND_FACTOR = 0.05;\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  // first filtering pass (step size = 1)\n  GBuffer g = psvgf(iChannel0, ivec2(fragCoord), 1);\n\n  // recreate the ray for this pixel from the camera data\n  CameraData camera = unpackCameraData(texelFetch(iChannel0, ivec2(0, 0), 0));\n  mat4 cameraMatrix = getInvViewMatrix(camera);\n  vec3 ro = camera.position;\n  vec3 rd = rayDirection(55.0, iResolution.xy, fragCoord);\n  rd = (cameraMatrix * vec4(rd, 0.0)).xyz;\n\n  // fetch the camera data from the previous frame\n  // we'll use it to reproject the pixel onto the history buffer\n  CameraData prevCamera =\n      unpackCameraData(texelFetch(iChannel0, ivec2(1, 0), 0));\n\n  // view matrix from previous frame\n  mat4 prevView = getViewMatrix(prevCamera);\n\n  // projection matrix from previous frame\n  mat4 prevProj = getProjMatrix(55.0, iResolution.xy, 1.0, 2.0);\n\n  // reconstruct world-space position from ray and depth\n  vec3 worldPos = ro + rd * g.depth;\n\n  // project world-space position to screen-space\n  vec3 projPos = project2Screen(prevView, prevProj, worldPos);\n\n  // fetch the reprojected pixel from history\n  GBuffer prevG = unpackGBuffer(\n      texelFetch(iChannel1, ivec2(projPos.xy * iResolution.xy), 0));\n\n  // bounds check\n  bvec4 inside = bvec4(projPos.x >= 0.0, projPos.y >= 0.0,\n                       projPos.x <= iResolution.x, projPos.y <= iResolution.y);\n\n  // if in bounds and not the first frame blend between the current frame and\n  // history buffer (section 4.2 from \"Progressive Spatiotemporal\n  // Variance-Guided Filtering\")\n  if (all(inside) && iFrame != 0) {\n    const float disocclusionFrames = 10.0;\n    const float disocclusionFactor = 5.0;\n\n    float disocclusion = abs(g.depth - prevG.depth);\n    if (disocclusion < disocclusionFactor) {\n      // increment the pixel's age\n      g.age = saturate(prevG.age + 1.0 / disocclusionFrames);\n\n      // r = blending factor\n      float r = max(HISTORY_BLEND_FACTOR, saturate(1.0 - g.age));\n\n      // mix the radiance and variance according to r\n      g.radiance = mix(prevG.radiance, g.radiance, r);\n      g.variance = mix(prevG.variance, g.variance, r);\n    } else {\n      // discard history value on disocclusion\n      g.age = 0.0;\n    }\n  } else {\n    g.age = 0.0;\n  }\n\n  fragColor = packGBuffer(g);\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Buffer B",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 258
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "// Second filtering pass (step size = 2)\n// See the psvgf() function in Common.\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, ivec2(fragCoord), 2);\n  fragColor = packGBuffer(g);\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Buffer C",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 259
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "// Third filtering pass (step size = 4).\n// See the psvgf() function in Common.\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord) {\n  GBuffer g = psvgf(iChannel0, ivec2(fragCoord), 4);\n  fragColor = packGBuffer(g);\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 259,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer02.png"
                    }
                ],
                "name": "Buffer D",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 260
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "// hack to prevent loop unrolling\n#define UZERO uint(min(0, iFrame))\n\n#ifndef saturate\n#define saturate(X) clamp(X, 0.0, 1.0)\n#endif\n\n#define PI 3.14159265359\n\n// [0..1] float to byte\nuint f2b(float value) { return uint(saturate(value) * 255.0) & 0xFFu; }\n// byte to [0..1] float\nfloat b2f(uint value) { return float(value & 0xFFu) * (1.0 / 255.0); }\n\n// 128-bit gbuffer\n//\n// albedo r (8), albedo g (8), albedo b (8), unused (8)\n// normal x (8), normal y (8), normal z (8), age (8)\n// depth (16), variance (16)\n// radiance (32)\n//\nstruct GBuffer {\n  vec3 albedo;\n  float radiance;\n  vec3 normal;\n  float depth;\n  float variance;\n  float age;\n};\n\n// Pack the GBuffer struct into a vec4.\nvec4 packGBuffer(GBuffer gbuf) {\n  uvec4 p;\n  p.x = f2b(gbuf.albedo.r) | f2b(gbuf.albedo.g) << 8 | f2b(gbuf.albedo.b) << 16;\n  vec3 normal = (gbuf.normal + 1.0) * 0.5;\n  p.y = f2b(normal.x) | f2b(normal.y) << 8 | f2b(normal.z) << 16 |\n        f2b(gbuf.age) << 24;\n  p.z = packHalf2x16(vec2(gbuf.depth, gbuf.variance));\n  p.w = floatBitsToUint(gbuf.radiance);\n  return uintBitsToFloat(p);\n}\n\n// Unpack the GBuffer struct from a vec4.\nGBuffer unpackGBuffer(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  GBuffer gbuf;\n  gbuf.albedo.r = b2f(p.x);\n  gbuf.albedo.g = b2f(p.x >> 8);\n  gbuf.albedo.b = b2f(p.x >> 16);\n  gbuf.normal.x = b2f(p.y);\n  gbuf.normal.y = b2f(p.y >> 8);\n  gbuf.normal.z = b2f(p.y >> 16);\n  gbuf.normal = normalize(gbuf.normal * 2.0 - 1.0);\n  gbuf.age = b2f(p.y >> 24);\n  vec2 tmp = unpackHalf2x16(p.z);\n  gbuf.depth = tmp.x;\n  gbuf.variance = tmp.y;\n  gbuf.radiance = uintBitsToFloat(p.w);\n  return gbuf;\n}\n\n// Sample a gbuffer texture.\nGBuffer sampleGBuffer(sampler2D tex, ivec2 uv) {\n  return unpackGBuffer(texelFetch(tex, uv, 0));\n}\n\n// Creates a 4x4 rotation matrix given an axis and and an angle.\nmat4 rotationMatrix(vec3 axis, float angle) {\n  axis = normalize(axis);\n  float s = sin(angle);\n  float c = cos(angle);\n  float oc = 1.0 - c;\n\n  return mat4(\n      oc * axis.x * axis.x + c, oc * axis.x * axis.y - axis.z * s,\n      oc * axis.z * axis.x + axis.y * s, 0.0, oc * axis.x * axis.y + axis.z * s,\n      oc * axis.y * axis.y + c, oc * axis.y * axis.z - axis.x * s, 0.0,\n      oc * axis.z * axis.x - axis.y * s, oc * axis.y * axis.z + axis.x * s,\n      oc * axis.z * axis.z + c, 0.0, 0.0, 0.0, 0.0, 1.0);\n}\n\n// Camera parameters.\nstruct CameraData {\n  vec3 position;\n  vec2 pitchYaw;\n  vec2 prevMouse;\n};\n\n// Pack the CameraData struct into a vec4.\nvec4 packCameraData(CameraData camera) {\n  uvec4 packed;\n  packed.x = packHalf2x16(camera.position.xy);\n  packed.y = packHalf2x16(vec2(camera.position.z));\n  packed.z = packHalf2x16(camera.pitchYaw);\n  packed.w = packHalf2x16(camera.prevMouse);\n  return uintBitsToFloat(packed);\n}\n\n// Unpack the CameraData struct from a vec4.\nCameraData unpackCameraData(vec4 packed) {\n  uvec4 p = floatBitsToUint(packed);\n\n  CameraData camera;\n  camera.position.xy = unpackHalf2x16(p.x);\n  camera.position.z = unpackHalf2x16(p.y).x;\n  camera.pitchYaw = unpackHalf2x16(p.z);\n  camera.prevMouse = unpackHalf2x16(p.w);\n  return camera;\n}\n\n// Returns the inverse view matrix for a camera.\nmat4 getInvViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, camera.position, 1.0);\n\n  return yaw * pitch * translate;\n}\n\n// Returns the view matrix for a camera.\nmat4 getViewMatrix(CameraData camera) {\n  mat4 pitch = rotationMatrix(vec3(1.0, 0.0, 0.0), -camera.pitchYaw.x);\n  mat4 yaw = rotationMatrix(vec3(0.0, 1.0, 0.0), -camera.pitchYaw.y);\n  mat4 translate = mat4(1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0,\n                        0.0, -camera.position, 1.0);\n\n  return pitch * yaw * translate;\n}\n\n// Returns a perspective projection matrix.\nmat4 getProjMatrix(float fov, vec2 size, float near, float far) {\n  float fn = far + near;\n  float f_n = far - near;\n  float r = size.x / size.y;\n  float t = -1.0 / tan(radians(fov) * 0.5);\n\n  return mat4(t / r, 0.0, 0.0, 0.0, 0.0, t, 0.0, 0.0, 0.0, 0.0, fn / f_n, 1.0,\n              0.0, 0.0, (2.0 * far * near) / f_n, 0.0);\n}\n\n// Calculates the ray direction in view space for a pixel given the camera's\n// field of view and the screen size in pixels.\nvec3 rayDirection(float fov, vec2 size, vec2 fragCoord) {\n  vec2 xy = fragCoord - size * 0.5;\n  float z = size.y / tan(radians(fov) * 0.5);\n  return normalize(vec3(xy, -z));\n}\n\n// Projects a world-space position to screen-space given camera view and\n// projection matrices.\nvec3 project2Screen(const mat4 view, const mat4 proj, vec3 v) {\n  vec4 p = proj * (view * vec4(v, 1.0));\n  p /= p.w;\n  p.xy += 0.5;\n  p.z *= 2.0;\n  p.z -= 1.0;\n  return p.xyz;\n}\n\n// Normal-weighting function (4.4.1)\nfloat normalWeight(vec3 normal0, vec3 normal1) {\n  const float exponent = 64.0;\n  return pow(max(0.0, dot(normal0, normal1)), exponent);\n}\n\n// Depth-weighting function (4.4.2)\nfloat depthWeight(float depth0, float depth1, vec2 grad, vec2 offset) {\n  // paper uses eps = 0.005 for a normalized depth buffer\n  // ours is not but 0.1 seems to work fine\n  const float eps = 0.1;\n  return exp((-abs(depth0 - depth1)) / (abs(dot(grad, offset)) + eps));\n}\n\n// Luminance-weighting function (4.4.3)\nfloat luminanceWeight(float lum0, float lum1, float variance) {\n  const float strictness = 4.0;\n  const float eps = 0.01;\n  return exp((-abs(lum0 - lum1)) / (strictness * variance + eps));\n}\n\n// 3x3 kernel from \"Progressive Spatiotemporal Variance-Guided Filtering\"\n// different kernels could potentially give better results\nconst float psvgfKernel[] =\n    float[](0.0625, 0.125, 0.0625, 0.125, 0.25, 0.125, 0.0625, 0.125, 0.0625);\n\nfloat psvgfWeight(GBuffer g, GBuffer s, vec2 dgrad, ivec2 offset,\n                  int stepSize) {\n  // calculate the normal, depth and luminance weights\n  float nw = normalWeight(g.normal, s.normal);\n  float dw = depthWeight(g.depth, s.depth, dgrad, vec2(offset));\n  float lw = luminanceWeight(g.radiance, s.radiance, g.variance);\n\n  // combine them with the kernel value\n  return saturate(nw * dw * lw) *\n         psvgfKernel[(offset.x / stepSize + 1) + (offset.y / stepSize + 1) * 3];\n}\n\n// The next function implements the filtering method described in the two papers\n// linked below.\n//\n// \"Progressive Spatiotemporal Variance-Guided Filtering\"\n// https://pdfs.semanticscholar.org/a81a/4eed7f303f7e7f3ca1914ccab66351ce662b.pdf\n//\n// \"Edge-Avoiding Ã€-Trous Wavelet Transform for fast Global Illumination\n// Filtering\" https://jo.dreggn.org/home/2010_atrous.pdf\n//\nGBuffer psvgf(sampler2D buf, ivec2 uv, int stepSize) {\n  GBuffer g = sampleGBuffer(buf, uv);\n\n  // depth-gradient estimation from screen-space derivatives\n  vec2 dgrad = vec2(dFdx(g.depth), dFdy(g.depth));\n\n  ivec3 d = ivec3(-1, 0, 1) * stepSize;\n\n  vec4 s00 = texelFetch(buf, uv + d.xx, 0);\n  vec4 s01 = texelFetch(buf, uv + d.xy, 0);\n  vec4 s02 = texelFetch(buf, uv + d.xz, 0);\n  vec4 s10 = texelFetch(buf, uv + d.yx, 0);\n  vec4 s12 = texelFetch(buf, uv + d.yz, 0);\n  vec4 s20 = texelFetch(buf, uv + d.zx, 0);\n  vec4 s21 = texelFetch(buf, uv + d.zy, 0);\n  vec4 s22 = texelFetch(buf, uv + d.zz, 0);\n\n  // blur the variance\n  float variance = 0.0;\n  variance += unpackHalf2x16(floatBitsToUint(s00.z)).y * psvgfKernel[0];\n  variance += unpackHalf2x16(floatBitsToUint(s01.z)).y * psvgfKernel[1];\n  variance += unpackHalf2x16(floatBitsToUint(s02.z)).y * psvgfKernel[2];\n  variance += unpackHalf2x16(floatBitsToUint(s10.z)).y * psvgfKernel[3];\n  variance += g.variance * psvgfKernel[4];\n  variance += unpackHalf2x16(floatBitsToUint(s12.z)).y * psvgfKernel[5];\n  variance += unpackHalf2x16(floatBitsToUint(s20.z)).y * psvgfKernel[6];\n  variance += unpackHalf2x16(floatBitsToUint(s21.z)).y * psvgfKernel[7];\n  variance += unpackHalf2x16(floatBitsToUint(s22.z)).y * psvgfKernel[8];\n  g.variance = variance;\n\n  // filtered radiance\n  float radiance = 0.0;\n\n  // weights sum\n  float wsum = 0.0;\n\n  GBuffer s = unpackGBuffer(s00);\n  float w = psvgfWeight(g, s, dgrad, d.xx, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s01);\n  w = psvgfWeight(g, s, dgrad, d.xy, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s02);\n  w = psvgfWeight(g, s, dgrad, d.xz, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s10);\n  w = psvgfWeight(g, s, dgrad, d.yx, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  radiance += g.radiance * psvgfKernel[4];\n  wsum += psvgfKernel[4];\n\n  s = unpackGBuffer(s12);\n  w = psvgfWeight(g, s, dgrad, d.yz, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s20);\n  w = psvgfWeight(g, s, dgrad, d.zx, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s21);\n  w = psvgfWeight(g, s, dgrad, d.zy, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  s = unpackGBuffer(s22);\n  w = psvgfWeight(g, s, dgrad, d.zz, stepSize);\n  radiance += s.radiance * w;\n  wsum += w;\n\n  // scale total radiance by the sum of the weights\n  g.radiance = radiance / wsum;\n\n  return g;\n}\n",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            }
        ],
        "ver": "0.1"
    }
}