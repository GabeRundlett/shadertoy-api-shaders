{
    "Shader": {
        "info": {
            "date": "1608326238",
            "description": "Added smoothing and allowed different scale cubes",
            "flags": 48,
            "hasliked": 0,
            "id": "3sGBWt",
            "likes": 3,
            "name": "Sphere Fractal Cave Test 4",
            "published": 3,
            "tags": [
                "fractal",
                "cave",
                "fly"
            ],
            "usePreview": 0,
            "username": "sdfgeoff",
            "viewed": 376
        },
        "renderpass": [
            {
                "code": "/// Convert from a buffer of normals/depth into something pretty\n#define BUFFER_WORLD iChannel0\n#define BUFFER_STATE iChannel1\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    \n    //vec2 screen_coords = (fragCoord/iResolution.xy);\n    //vec4 world = texture(WORLD_BUFFER, screen_coords);\n    vec2 uv = (fragCoord/iResolution.xy);\n    \n    // sample previous tile\n    #ifdef HQ\n    vec4 previous = vec4(0.0);\n    #else\n    vec4 previous = sample_tile(BUFFER_WORLD, iResolution.xy, uv, 1.0);\n    #endif\n    \n    mat4 camera_transform = quat_to_transform(\n        read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n        read_data(BUFFER_STATE, ADDR_CAMERA_POSITION).xyz\n    );\n    \n\n    // Output to screen\n    vec4 data = raymarch(\n        iResolution.xy,\n        uv,\n        previous,\n        camera_transform\n    );\n    \n    float dist = data.a;\n    vec3 end_position = data.xyz;\n    \n    vec3 surface_normal = calc_normal(end_position);\n    \n    float lighting = dot(surface_normal, vec3(0.5)) * 0.5 + 0.5;\n    \n    float fog = dist * 0.25;//pow(dist, 2.0);\n    \n    vec3 col = vec3(0.5, 0.6, 0.9);\n    col *= lighting;\n    col = mix(col, vec3(1.0, 0.9, 0.8), fog);\n    \n    fragColor = vec4(col, 1.0);\n    \n    if (iMouse.z > 0.0) {\n        //fragColor = sample_tile(iChannel0, iResolution.xy, uv, 1.0);\n        fragColor = vec4(texture(BUFFER_WORLD, uv).a) * 0.1;\n    }\n}\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "// STATE\n\n#define BUFFER_STATE iChannel0\n#define BUFFER_KEYBOARD iChannel1\n\n\n// Return the state of a key\nfloat get_key(int key_code) {\n    return texelFetch(BUFFER_KEYBOARD, ivec2(key_code,0), 0).x;\n}\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    ivec2 address = ivec2(fragCoord);\n    \n    if (address == ADDR_CAMERA_POSITION) {\n        // Move the camera based on keypress\n    \tvec4 camera_position = read_data(BUFFER_STATE, ADDR_CAMERA_POSITION);\n        \n        if (iTime < 0.1) {\n            camera_position = vec4(0.0, 0.0, 7.0, 0.0);\n        }\n        \n        \n        vec3 translation = read_data(BUFFER_STATE, ADDR_CAMERA_LIN_VELOCITY).xyz;\n        \n        // Convert to local coordinate system\n        mat4 orientation = quat_to_transform(\n            read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n            vec3(0.0)\n        );\n        translation = (orientation * vec4(translation, 0.0)).xyz;\n        translation *= iTimeDelta;\n        \n        camera_position.xyz += translation;\n        fragColor = camera_position;\n        return;\n    }\n    \n    \n    if (address == ADDR_CAMERA_ORIENTATION) {\n        // Rotate the camera based on keypress\n        vec4 camera_orientation = read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION);\n        \n        if (iTime < 0.1) {\n            camera_orientation = quat_from_axis_angle(vec3(0.0, 1.0, 0.0), 0.3);\n        }\n        \n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_ANG_VELOCITY);\n        velocity *= iTimeDelta;\n        \n        \n        \n        vec4 pan = quat_from_axis_angle(vec3(0.0, 1.0, 0.0), velocity.x);\n        vec4 tilt = quat_from_axis_angle(vec3(1.0, 0.0, 0.0), velocity.y);\n        vec4 roll = quat_from_axis_angle(vec3(0.0, 0.0, 1.0), velocity.z);\n        \n        \n        camera_orientation = quat_mul(pan, camera_orientation); \n        camera_orientation = quat_mul(tilt, camera_orientation); \n        camera_orientation = quat_mul(roll, camera_orientation); \n        \n        fragColor = camera_orientation;\n        return;\n    }\n    if (address == ADDR_CAMERA_ANG_VELOCITY) {\n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_ANG_VELOCITY);\n        \n        vec3 acceleration = vec3(\n            get_key(KEY_PAN_LEFT) - get_key(KEY_PAN_RIGHT),\n            get_key(KEY_TILT_UP) - get_key(KEY_TILT_DOWN),\n            get_key(KEY_ROLL_RIGHT) - get_key(KEY_ROLL_LEFT)\n        ) * 10.0;\n        velocity.xyz += acceleration * iTimeDelta;\n        \n        vec4 drag = velocity * 10.0;\n        velocity -= drag * iTimeDelta;\n        \n        fragColor = velocity;\n        return;\n    }\n    if (address == ADDR_CAMERA_LIN_VELOCITY) {\n        vec4 velocity = read_data(BUFFER_STATE, ADDR_CAMERA_LIN_VELOCITY);\n        \n        vec3 acceleration = vec3(\n            get_key(KEY_RIGHT) - get_key(KEY_LEFT),\n            get_key(KEY_UP) - get_key(KEY_DOWN),\n            get_key(KEY_FORWARD) - get_key(KEY_BACKWARD)\n        ) * 5.0;\n        velocity.xyz += acceleration * iTimeDelta;\n        \n        vec4 drag = velocity * 5.0;\n        velocity -= drag * iTimeDelta;\n        \n        fragColor = velocity;\n        return;\n    }\n}\n\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 1,
                        "ctype": "keyboard",
                        "id": 33,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/presets/tex00.jpg"
                    },
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "#define HQ\n\nconst ivec2 ADDR_CAMERA_POSITION = ivec2(0,0);\nconst ivec2 ADDR_CAMERA_ORIENTATION = ivec2(0,1);\nconst ivec2 ADDR_CAMERA_ANG_VELOCITY = ivec2(0,2);\nconst ivec2 ADDR_CAMERA_LIN_VELOCITY = ivec2(0,3);\n\nconst float LENS = 0.5;\n\nconst int KEY_LEFT = 65;\nconst int KEY_UP   = 82;\nconst int KEY_RIGHT = 68;\nconst int KEY_DOWN = 70;\nconst int KEY_FORWARD = 87;\nconst int KEY_BACKWARD = 83;\n\nconst int KEY_TILT_UP = 38;\nconst int KEY_TILT_DOWN = 40;\nconst int KEY_PAN_LEFT = 37;\nconst int KEY_PAN_RIGHT = 39;\nconst int KEY_ROLL_LEFT = 81;\nconst int KEY_ROLL_RIGHT = 69;\n\nconst float MAX_TILES = 3.0;\n\n#ifdef HQ\nconst int steps = 96;\n#else\nconst int steps = 32;\n#endif\n\n// Fetch a single pixe from a buffer\nvec4 read_data(sampler2D buffer, ivec2 address){\n    return texelFetch(buffer, address, 0);\n}\n\n\n\n// A surface is defined by a vec4. The rgb channels are the normal, the alpha\n// is the distance to the surface. This makes physics etc. very easy\nfloat sphere_sdf(vec3 query_point, float sphere_radius) {\n    float l = length(query_point);\n    float df = l - sphere_radius;\n    \n    return df;\n}\n\n\nfloat sdRoundBox( vec3 p, vec3 b, float r )\n{\n  vec3 q = abs(p) - b;\n  return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0) - r;\n}\n\n\n\nfloat surface_intersect(float surface_1, float surface_2) {\n    // Find the volume both surface occupy\n    return max(surface_1, surface_2);\n}\nfloat surface_difference(float surface_1, float surface_2) {\n    // subtract surface_2 from surface_1\n    // invert surface 2:\n    return surface_intersect(surface_1, -surface_2);\n}\n\n\n// polynomial smooth min (k = 0.1);\nfloat smin( float a, float b, float k )\n{\n    float h = max( k-abs(a-b), 0.0 )/k;\n    return min( a, b ) - h*h*k*(1.0/4.0);\n}\n\n// polynomial smooth min (k = 0.1);\nfloat smax( float a, float b, float k )\n{\n    float h = max( k-abs(a-b), 0.0 )/k;\n    return max( a, -b ) + h*h*k*(1.0/4.0);\n}\n\n\n\nfloat distance_field(vec3 co) {\n    float rad = clamp(co.z * 0.05 + 0.45, 0.1, 0.3);\n    //float rad = 0.45;\n    co = mod(co, vec3(1.0)) - 0.5;\n    //co = (co - clamp(round(co), -5.0, 5.0));\n    //return sphere_sdf(co, rad);\n    \n    return sdRoundBox(co, vec3(rad, rad, 0.3), 0.1);\n}\n\n\n// A simple fractal-like made by iterative transformation\nfloat sample_world(vec3 world_position) {\n\n    float body = 999.0;\n    vec3 co = world_position;\n    //co.z *= 0.5;\n    \n    float scale = 0.2;\n    mat4 m = mat4(\n\t\tvec4(0.6373087, -0.0796581,  0.7664804, 0.0),\n  \t\tvec4(0.2670984,  0.9558195, -0.1227499, 0.0),\n  \t\tvec4(-0.7228389,  0.2829553,  0.6304286, 0.0),\n        vec4(0.1, 0.6, 0.2, 0.0)\n    );\n    //mat4 m = mat4(1.0);\n    \n    for (int i=0; i<3; i++) {\n        co = (m * vec4(co, float(i))).xyz;\n        scale *= (3.0);\n        \n        float field = distance_field(co * scale) / scale;\n     \tbody = smin(body, field, 0.05);\n    }\n    \n    return -body;\n}\n\nvec3 calc_normal(vec3 sample_point) {\n    const float h = 0.01; // replace by an appropriate value\n    const vec2 k = vec2(1,-1);\n    \n    vec3 normal = normalize(\n\t\tk.xyy * sample_world( sample_point + k.xyy*h ) + \n\t\tk.yyx * sample_world( sample_point + k.yyx*h ) + \n\t\tk.yxy * sample_world( sample_point + k.yxy*h ) + \n\t\tk.xxx * sample_world( sample_point + k.xxx*h ) );\n    normal = normal.zyx;\n    return normal;\n}\n\n\nvec4 raymarch(vec2 resolution, vec2 uv, vec4 start_data, mat4 camera_transform) {\n    // Convert to range (-1, 1) and correct aspect ratio\n    vec2 screen_coords = (uv - 0.5) * 2.0;\n    screen_coords.x *= resolution.x / resolution.y;\n    \n    \n    vec3 ray_start_position = camera_transform[3].xyz;\n    \n    vec3 ray_direction = normalize(vec3(screen_coords * LENS, 1.0));\n    ray_direction = (camera_transform * vec4(ray_direction, 0.0)).xyz;\n    \n    \n    float dist = start_data.a * 0.9;\n    vec3 sample_point = ray_start_position + dist * ray_direction;\n    \n    float results = sample_world(sample_point);\n    \n    float tolerance = 0.0;\n    \n    for (int i=0; i<steps; i += 1) {\n        dist += results;\n        sample_point += ray_direction * results;\n        results = sample_world(sample_point);\n        \n        // TODO: Derive from resolution, camera lens and distance\n    \ttolerance = LENS / resolution.x * dist;\n        \n        if (results < tolerance || dist > 5.0) {\n        \tbreak; \n        }\n    }\n    \n    \n    \n    return vec4(sample_point, dist);\n}\n\n\n\n\n\n// Create a quaternion from axis-angle notation\nvec4 quat_from_axis_angle(vec3 axis, float angle) {\n    float factor = sin(angle) / 2.0;\n    float w = cos(angle) / 2.0;\n    return normalize(vec4(axis*factor, w));\n}\n\n// Convert a quaternion into a transformation matrix\nmat4 quat_to_transform(vec4 quat, vec3 translation) {\n    float qx = quat.x;\n    float qy = quat.y;\n    float qz = quat.z;\n    float qw = quat.w;\n    float qx2 = qx * qx;\n    float qy2 = qy * qy;\n    float qz2 = qz * qz;\n    \n \treturn mat4(\n        1.0 - 2.0*qy2 - 2.0*qz2,\t2.0*qx*qy - 2.0*qz*qw,\t2.0*qx*qz + 2.0*qy*qw, 0.0,\n    \t2.0*qx*qy + 2.0*qz*qw,\t1.0 - 2.0*qx2 - 2.0*qz2,\t2.0*qy*qz - 2.0*qx*qw, 0.0,\n    \t2.0*qx*qz - 2.0*qy*qw,\t2.0*qy*qz + 2.0*qx*qw,\t1.0 - 2.0*qx2 - 2.0*qy2, 0.0,\n        translation, 0.0\n    );\n}\n\n// Multiply two quaternions\nvec4 quat_mul(vec4 a, vec4 b) {\n \treturn vec4(\n        a.w * b.x + a.x * b.w + a.y * b.z - a.z * b.y,\n        a.w * b.y - a.x * b.z + a.y * b.w + a.z * b.x,\n        a.w * b.z + a.x * b.y - a.y * b.x + a.z * b.w,\n        a.w * b.w - a.x * b.x - a.y * b.y - a.z * b.z\n    );   \n}\n\n\n\n\n\n\nvec2 tile_resolution(vec2 screen_resolution, float tile_id_f) {\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n    return screen_resolution * tile_height;\n}\n\n\nvec4 sample_tile(sampler2D buffer, vec2 resolution, vec2 uv, float tile_id_f) {\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n    \n    uv.x += 1.0;\n    vec2 area_uv = uv * tile_height;\n    \n    // Compensate for GL.LINEAR sampling - we need to sample the middle of the pixel\n    vec2 tile_resolution = resolution * tile_height;\n    vec2 inv_resolution = 1.0 / resolution.xy;\n    area_uv -= mod(area_uv, inv_resolution) - inv_resolution * 0.5;\n    \n    return texture(buffer, area_uv);\n}\n\n",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            },
            {
                "code": "// Render the world. \n//\n// Outputs:\n// - rgb = analytic normal\n// - a = depth\n//\n// Pretty much a single raymarcher\n\n#define BUFFER_STATE iChannel0\n#define BUFFER_SELF iChannel1\n\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n#ifndef HQ\n    // Normalized pixel coordinates (from 0 to 1)\n    vec2 uv = fragCoord/iResolution.xy;\n    \n    float tile_id_f = floor(log2(1.0 / uv.x)) + 1.0;\n    float tile_height = 1.0 / pow(2.0, tile_id_f);\n\n\t// Convert to coordinates for this tile    \n    vec2 tile_uv = vec2(uv / tile_height);\n   \ttile_uv.x -= 1.0;\n    \n    if (uv.y > tile_height || tile_id_f > MAX_TILES) {\n        fragColor = vec4(0.0);\n        return;\n    }\n    \n    // sample previous tile\n    vec4 previous = sample_tile(BUFFER_SELF, iResolution.xy, tile_uv, tile_id_f + 1.0);\n    \n    \n    mat4 camera_transform = quat_to_transform(\n        read_data(BUFFER_STATE, ADDR_CAMERA_ORIENTATION),\n        read_data(BUFFER_STATE, ADDR_CAMERA_POSITION).xyz\n    );\n    \n    \n    vec4 data = raymarch(\n        tile_resolution(iResolution.xy, tile_id_f),\n        tile_uv,\n        previous,\n        camera_transform\n    );\n    \n    // Output to screen\n    fragColor = vec4(data);\n#endif\n}\n\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    }
                ],
                "name": "Buffer B",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 258
                    }
                ],
                "type": "buffer"
            }
        ],
        "ver": "0.1"
    }
}