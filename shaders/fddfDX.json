{
    "Shader": {
        "info": {
            "date": "1657340452",
            "description": "Rather janky ssgi I ported & modified to shadertoy, it could be much faster and better but I'm publishing it incase it might help someone else. ",
            "flags": 32,
            "hasliked": 0,
            "id": "fddfDX",
            "likes": 29,
            "name": "SSGI Cornell",
            "published": 3,
            "tags": [
                "cornell",
                "ssgi"
            ],
            "usePreview": 0,
            "username": "Tater",
            "viewed": 1445
        },
        "renderpass": [
            {
                "code": "\n//  Common: Scene and other functions\n//Buffer A: Normals and Depth \n//Buffer B: Direct Lighting\n//Buffer C: SSGI \n//Buffer D: Denoise 1\n//   Image: Denoise 2\n\n\n//Short Explanation:\n/*\nAlgorithm is by Alexander Sannikov, as implimented in \"Legit Engine\"\nwhich (I believe) is based on \"Horizon Based Ambient Occlusion\" \n\nhttps://developer.download.nvidia.com/presentations/2008/SIGGRAPH/HBAO_SIG08b.pdf\nhttps://github.com/Raikiri/LegitEngine\n\nEach pixel traverses the depth map in a fixed numbed of directions in screen space\n\nThe points along each direction are sampled with exponential sampling,\nat each sample the horizon angle is calculated, if the sample is visible then its \nlighting contribution is calculated base on currently visible horizon. \n\nThe samples from all directions are then combined to get the lighting\n*/\n\n//still needs:\n//a better denoiser\n//proper temporal reprojection\n//better sampling pattern\n//\"deinterleaved\" rendering? (could this even work on shadertoy?) \n//faster\n//sample rejection from normal to prevent bleeding (seems to cause artifacts) (also slow, extra tap D: )\n//dynamically pick mip level for sampling\n\n//Image shader\n//Denoise Pass 2\nvec3 fragCamPos = vec3(0);\nvec3 screenToWorld(vec3 screenPos)\n{\n  screenPos.xy -= 0.5;\n  screenPos.xy /= vec2(iResolution.y / iResolution.x, 1);\n  vec3 ro = fragCamPos;\n  mat3 camMat = getCamMatrix(ro,vec3(0));\n  vec3 rd = camMat*normalize(vec3(screenPos.xy , 1./FOV)); \n  return ro + screenPos.z*rd;\n}\n\n//Denoise is weighted based on worldspace distance and alignment of normals\nvec3 edgeDenoise(vec2 fragCoord)\n{\n    vec2 uv = fragCoord / iResolution.xy;\n    vec4 fragSample = texture(iChannel0, uv);\n    vec3 fragNormal = fragSample.xyz;\n    float fragDepth = fragSample.w;\n    vec3 fragPos = screenToWorld(vec3(uv, fragDepth));\n    float weight = 0.0;\n    vec3 col = vec3(0);\n    float kSize = kSize2;\n    for(float i = -kSize; i <=kSize; i++){\n        for(float j = -kSize; j <=kSize; j++){\n            vec2 sampleUV = (fragCoord + vec2(i,j)*jank)/iResolution.xy;\n            vec4 dSample = texture(iChannel0, sampleUV);\n            vec3 sampleNorm = dSample.xyz;\n            float sampleDepth = dSample.w;\n            vec3 samplePos = screenToWorld(vec3(sampleUV, sampleDepth));\n            float normAlignment = clamp(dot(sampleNorm, fragNormal), 0.0, 1.0);\n            float delta = distance(samplePos, fragPos) * 0.1;\n            vec3 sampleCol = texture(iChannel2, sampleUV).rgb;\n            float sampleWeight = normAlignment / (delta + 1e-2);\n            weight += sampleWeight;\n            col += sampleCol * sampleWeight;\n        }\n    }\n    return col/weight; \n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ){\n    //Init camera position\n    fragCamPos = getCamPos(iTime,iMouse,iResolution);\n    \n    #ifdef DENOISE\n    //Get denoised image\n    fragColor.rgb = edgeDenoise(fragCoord);\n    #else\n    fragColor.rgb = texture(iChannel3,fragCoord/iResolution.xy).rgb;\n    #endif\n    //Show direct lighting on screen\n    fragColor += texture(iChannel1,fragCoord/iResolution.xy);\n    \n    //Gamma Correction\n    fragColor = pow(fragColor, vec4(1.0 / 2.2));\n}\n\n\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    },
                    {
                        "channel": 3,
                        "ctype": "buffer",
                        "id": 259,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer02.png"
                    },
                    {
                        "channel": 2,
                        "ctype": "buffer",
                        "id": 260,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer03.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            },
            {
                "code": "//Common Functions\n\n#define DENOISE\n\n//Defines if previous frame lighting is used to approximate bounce light\n#define BOUNCE\nconst float absorbtion = 0.3;\n\nconst float FOV = 0.95;\n\n//Camera target/look-at position\nconst vec3 look = vec3(0,0,0);\n\n//Blur/denoise kernal sizes\nconst float kSize1 = 5.0;\nconst float kSize2 = 4.0;\nconst float jank = 3.;\n\nconst float pi = 3.1415f;\n\nfloat linstep(float a, float b, float x){\n    return clamp((x - a)/(b - a), 0., 1.); \n}\n\nmat3 getCamMatrix(vec3 ro, vec3 look) {\n    vec3 f = normalize(ro - look);\n    vec3 r = normalize(cross(vec3(0, 1, 0), f)); \n    return mat3(-r, normalize(cross(f, r)), -f);\n}\n\n//2d rotation matrix\nmat2 rot(float a){\n    return mat2(cos(a),sin(a),-sin(a),cos(a));\n}\n\nvec3 getCamPos(float iTime, vec4 iMouse, vec3 iResolution)\n{\n    vec3 ro = vec3(0.01,0.06,12.4);\n    if(iMouse.z>0.){\n       ro.yz*=rot(2.0*(iMouse.y/iResolution.y-0.5));\n       ro.zx*=rot(-9.0*(iMouse.x/iResolution.x-0.5));\n    }\n    return ro;\n}\n\n//Blackle hash (CC0) \n#define FK(k) floatBitsToInt(cos(k))^floatBitsToInt(k)\nfloat hash(vec2 p) {\n  int x = FK(p.x); int y = FK(p.y);\n  return float((x-y*y)*(x*x+y)-x)/2.14e9;\n}\n\n\n//iq box \nfloat box( vec3 p, vec3 b )\n{\n  vec3 q = abs(p) - b;\n  return length(max(q,0.0)) + min(max(q.x,max(q.y,q.z)),0.0);\n}\n\n//SDF of the scene\nfloat map(vec3 p)\n{\n    vec3 po = p;\n    float a = 0.;\n    a = -box(p,vec3(2,2,2)*2.);\n    a = max(p.z-3.9,a);\n    p-=vec3(-1.7,-3,1.7);\n    p.xz*=rot(-0.4);\n    a = min(box(p,vec3(1)*1.15),a);\n    p = po;\n    p-=vec3(1.2,-3,-1.);\n    p.xz*=rot(0.5);\n    a = min(box(p,vec3(1,3,1)*1.25),a);  \n    p = po;\n    a = max(box(p,vec3(2,2,2)*2.3),a);\n\n    return a;\n\n}\n\n//Simple way to get the color of walls\nvec3 getColor(vec3 p){\n    if(p.x>3.99)\n        return vec3(1.0,0.0,0.0);\n    if(p.x<-3.99)\n        return vec3(0.0,1.0,0.0);\n    return vec3(1.0);\n    \n}\n\nvec3 norm(vec3 p){\n    vec2 e = vec2(0.01,0.);\n    return normalize(map(p)-vec3(\n    map(p-e.xyy),\n    map(p-e.yxy),\n    map(p-e.yyx)));\n}\n\n",
                "description": "",
                "inputs": [],
                "name": "Common",
                "outputs": [],
                "type": "common"
            },
            {
                "code": "//Buffer A\n//Depth and Normals buffer\n\n//This buffer raymarches the SDF in common for depth and normals\n#define MDIST 150.0\n#define STEPS 128.0\n#define pi 3.1415926535\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ){\n    vec3 col = vec3(0);\n    vec2 uv = (fragCoord-0.5*iResolution.xy)/iResolution.y;\n    \n    vec3 ro = getCamPos(iTime,iMouse,iResolution);\n    \n    //create camera matrix and get ray direction\n    mat3 camMat = getCamMatrix(ro,look);\n    vec3 rd = camMat*normalize(vec3(uv, 1./FOV)); \n\n    //setup sdf raymarcher\n    vec3 p = ro;\n    float rl = 0.0; \n    vec3 d;\n    bool hit = false;\n    for(float i = 0.; i<STEPS; i++){\n        p = ro+rd*rl;\n        d = map(p)*vec3(1);\n        rl+=d.x;\n        if(abs(d.x)<0.001){\n            hit = true;\n            break;\n        }\n        if(rl>MDIST)\n            break;\n    }\n    vec3 n = norm(p);\n    fragColor = vec4(n,rl);\n}",
                "description": "",
                "inputs": [],
                "name": "Buffer A",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 257
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "//Buffer B\n//Direct lighting buffer\n\n//Just some messy code to put a square on the top of the box using depth buffer\nfloat maxNormDist(vec3 p){\n    vec3 d = abs(p);\n    return max(d.x,max(d.y,d.z));\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    vec2 uv = (fragCoord-0.5*iResolution.xy)/iResolution.y;\n    vec3 ro = getCamPos(iTime,iMouse,iResolution);\n    mat3 camMat = getCamMatrix(ro,look);\n    float time = iTime * 0.8;\n    vec3 rd = camMat * normalize(vec3(uv, 1./FOV)); \n    float depth = texture(iChannel0,fragCoord/iResolution.xy).w;\n    \n    vec3 pos = ro + depth * rd;\n\n    vec3 light = vec3(0.992,1.000,0.639) * 8.0 * \n    smoothstep(0., 0.1, -maxNormDist(pos + vec3(cos(time) * 0.6, -3.0, sin(time)*0.6)) + 2.0);\n    if(pos.y<3.99 || pos.y> 4.05)\n    light-=light;\n    fragColor = vec4(vec3(light),1.);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    }
                ],
                "name": "Buffer B",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 258
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "//Buffer C\n//Screen Space GI pass\n//\n//Heavily Modified/Simplied/Partially re-written version of SSGI by Alexander Sannikov from \"Legit Engine\"\n//\n//Alexander Sannikov\n//MIT License\n//https://github.com/Raikiri/LegitEngine\n//\n\n\n//Sample directions controls noise\nconst int sampleDirs = 6;\n\n//Sample density controls accuracy \nconst float sampleDensity = 0.75; //0.0-0.9\n\nconst float ambientLightAmount = 0.001;\nconst float edgeFadeWidth = 0.1;\n\nvec3 fragCamPos = vec3(0);\n\nvec3 screenToWorld(vec3 screenPos)\n{\n  screenPos.xy -= 0.5;\n  screenPos.xy /= vec2(iResolution.y / iResolution.x, 1);\n  vec3 ro = fragCamPos;\n  mat3 camMat = getCamMatrix(ro,vec3(0));\n  vec3 rd = camMat*normalize(vec3(screenPos.xy , 1./FOV)); \n  return ro + screenPos.z*rd;\n}\n\nfloat HorizonContribution(vec3 rayDir, vec3 worldTangent, vec3 viewNorm, float minAngle, float maxAngle)\n{\n    minAngle *= 2.0; \n    maxAngle *= 2.0;\n    return 0.25 * (dot(rayDir, viewNorm) * (-cos(maxAngle) + cos(minAngle)) + dot(worldTangent, viewNorm) * (maxAngle - minAngle - sin(maxAngle) + sin(minAngle)));\n}\n\nvoid mainImage(out vec4 fragColor, in vec2 fragCoord)\n{\n    #ifdef JITTERUV\n    fragCoord += (vec2(hash(vec2(iTime)),hash(vec2(iTime+0.5)))-0.5);\n    #endif\n    vec2 uv = fragCoord / iResolution.xy;\n\n    vec3 fragNorm  = texture(iChannel0, uv).xyz;\n    float fragDepth = texture(iChannel0, uv,1.0).w;\n    \n    fragCamPos = getCamPos(iTime,iMouse,iResolution);\n    vec3 fragWorldPos = screenToWorld(vec3(uv, fragDepth));\n    vec3 rayDir = normalize(fragCamPos - fragWorldPos);\n\n\n    //Random rotation and linear jitter for all directions per pixel\n    //TODO: try using some low discrepancy noise/sequence\n    float seed = mod(float(iFrame),10.0); //hack to get temporally stable result :)\n    float angleOffset = hash(uv+seed) * pi * 2.0;\n    float linearOffset = hash(uv+angleOffset);\n\n    float exponentialDensity = 1.0 / sampleDensity;\n    float maxPixelDist = max(iResolution.x,iResolution.y);\n\n    vec3 indirectLight = vec3(0);\n    vec3 ambientLight = vec3(0);\n    \n    //Sample from multiple linear screenspace directions\n    for(int dirIndex = 0; dirIndex < sampleDirs; dirIndex++)\n    {\n        //get direction vector for samples in this direction \n        float dirAngle = 2.0 * pi / float(sampleDirs) * (float(dirIndex) + angleOffset);\n        vec2 dirVector = vec2(cos(dirAngle), sin(dirAngle));\n        \n        //offset world pos to get tangent vector\n        vec3 offsetWorldPos = screenToWorld(vec3((fragCoord + dirVector) / iResolution.xy, fragDepth));\n        vec3 worldTangent = normalize(rayDir + normalize(offsetWorldPos - fragCamPos));\n        \n        //Get 2d normal in the plane of the current direction\n        vec3 dirNormalPos = cross(-cross(worldTangent, rayDir), fragNorm);\n        \n        //Initial direction of horizon angle\n        vec2 projFront = vec2(dot(rayDir, dirNormalPos), dot(worldTangent, dirNormalPos));\n        \n        //ititalize max horizon angle\n        float maxAngle = atan(projFront.y, projFront.x);\n        \n        vec3 dirLight = vec3(0);\n        vec3 dirAmbient = ambientLightAmount * HorizonContribution(rayDir, worldTangent, fragNorm, 0.0, maxAngle) * getColor(fragWorldPos) * vec3(1);\n        \n        int sampleCount = int(log(maxPixelDist)/log(exponentialDensity)) + 2;\n        \n        //Sample a series of points in the stright screenspace line\n        for(int sampleIndex = 0; sampleIndex < sampleCount; sampleIndex++)\n        {\n            //Move offset along the direction with exponential sampling pattern to get\n            //screen space sample position\n            float pixelOffset = pow(exponentialDensity, float(sampleIndex) + linearOffset);\n            pixelOffset = max(pixelOffset,float(sampleIndex)*2.+1.);\n            //pixelOffset = float(sampleIndex)*7.+1. + linearOffset;\n            vec2 sampleUV = (fragCoord + dirVector * pixelOffset) / iResolution.xy;\n            \n            //If sample is offscreen then stop sampling this direction\n            if(max(sampleUV.x, sampleUV.y) > 1.0 || min(sampleUV.x, sampleUV.y) < 0.0)break;\n\n            //Calculate \"edgeFade\" to reduce intensity of light near edge of screen (avoids distracting popping)\n            vec2 d = abs(sampleUV - 0.5);\n            float mult = max(d.x, d.y);\n            float edgeFade = linstep(0.5, 0.5 - max(edgeFadeWidth,0.01), mult);\n            \n            //Sample depth and get sample horizon angle\n            float sampleDepth = textureLod(iChannel0, sampleUV.xy, 1.0).w;\n            vec3 sampleWorldPos = fragCamPos + normalize(screenToWorld(vec3(sampleUV, 1.0)) - fragCamPos) * sampleDepth;\n            vec3 worldDir = normalize(sampleWorldPos - fragWorldPos);\n            vec2 projTangetDir = vec2(dot(rayDir, worldDir), dot(worldTangent, worldDir));\n            float sampleAngle = atan(projTangetDir.y, projTangetDir.x);\n            \n            //If sampled world space position is visible then find its contribution\n            if(sampleAngle < maxAngle)\n            {\n                //Get direct lighting (sampling in higher mip to greatly improve perf, could be dynamic)\n                vec3 directLight = textureLod(iChannel1, sampleUV.xy, 3.0).rgb;\n\n                \n                //Optionaly include indirect lighting from previous frame to approximate multiple bounces\n                float absorb = 0.0;\n                #ifdef BOUNCE\n                absorb = absorbtion;\n                directLight += edgeFade * max(absorb * textureLod(iChannel2, sampleUV.xy, 3.0), 0.0).rgb;\n                #endif \n                float sampleContribution = (1.0 - absorb) * edgeFade * HorizonContribution(rayDir, worldTangent, fragNorm, sampleAngle, maxAngle);\n                \n                maxAngle = sampleAngle;\n                \n                //Add to indirect lighting and subtract from ambient\n                dirLight += directLight * sampleContribution * getColor(fragWorldPos);\n                dirAmbient -= ambientLightAmount * sampleContribution;\n            }\n        }\n        indirectLight += (dirLight) / float(sampleDirs) * 2.0;\n        ambientLight += (dirAmbient) / float(sampleDirs) * 2.0;\n    }\n    \n    //Combining ambient light with indirect light for simplicity but they could be kept seperate\n    //for more accuracy \n    indirectLight+=vec3(ambientLight);\n    \n    fragColor.rgb = indirectLight;\n\n    //Accumulate a few past frames\n    //This should probably use temporal reprojection but not enough buffers\n    \n    //if(iMouse.z<0.5)\n        fragColor.rgb = mix(texture(iChannel2,fragCoord/iResolution.xy).rgb, indirectLight, 0.15);\n        \n    //Clamp just for saftey\n    fragColor = clamp(fragColor,0.0,1.0);\n}\n\n\n\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "mipmap",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 258,
                        "published": 1,
                        "sampler": {
                            "filter": "mipmap",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer01.png"
                    },
                    {
                        "channel": 2,
                        "ctype": "buffer",
                        "id": 259,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer02.png"
                    }
                ],
                "name": "Buffer C",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 259
                    }
                ],
                "type": "buffer"
            },
            {
                "code": "//Buffer D\n//Denoise pass 1\n\nvec3 fragCamPos = vec3(0);\nvec3 screenToWorld(vec3 screenPos)\n{\n  screenPos.xy -= 0.5;\n  screenPos.xy /= vec2(iResolution.y / iResolution.x, 1);\n  vec3 ro = fragCamPos;\n  mat3 camMat = getCamMatrix(ro,vec3(0));\n  vec3 rd = camMat*normalize(vec3(screenPos.xy , 1./FOV)); \n  return ro + screenPos.z*rd;\n}\n\n//Denoise is weighted based on worldspace distance and alignment of normals\nvec3 edgeDenoise(vec2 fragCoord)\n{\n    vec2 uv = fragCoord / iResolution.xy;\n    vec4 fragSample = texture(iChannel0, uv);\n    vec3 fragNormal = fragSample.xyz;\n    float fragDepth = fragSample.w;\n    vec3 fragPos = screenToWorld(vec3(uv, fragDepth));\n    float weight = 0.0;\n    vec3 col = vec3(0);\n    float kSize = kSize1;\n    for(float i = -kSize; i <=kSize; i++){\n        for(float j = -kSize; j <=kSize; j++){\n            vec2 sampleUV = (fragCoord + vec2(i,j))/iResolution.xy;\n            vec4 dSample = texture(iChannel0, sampleUV);\n            vec3 sampleNorm = dSample.xyz;\n            float sampleDepth = dSample.w;\n            vec3 samplePos = screenToWorld(vec3(sampleUV, sampleDepth));\n            float normAlignment = clamp(dot(sampleNorm, fragNormal), 0.0, 1.0);\n            float delta = distance(samplePos, fragPos) * 0.1;\n            vec3 sampleCol = texture(iChannel1, sampleUV).rgb;\n            float sampleWeight = normAlignment / (delta + 1e-2);\n            weight += sampleWeight;\n            col += sampleCol * sampleWeight;\n        }\n    }\n    return col/weight; \n}\n\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord ){\n    //Init camera position\n    fragCamPos = getCamPos(iTime,iMouse,iResolution);\n    //Get denoised image\n    #ifdef DENOISE\n    fragColor.rgb = edgeDenoise(fragCoord);\n    #endif\n    //Clamp just for saftey\n    fragColor = clamp(fragColor,0.0,1.0);\n}\n\n",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "buffer",
                        "id": 257,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer00.png"
                    },
                    {
                        "channel": 1,
                        "ctype": "buffer",
                        "id": 259,
                        "published": 1,
                        "sampler": {
                            "filter": "linear",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "clamp"
                        },
                        "src": "/media/previz/buffer02.png"
                    }
                ],
                "name": "Buffer D",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 260
                    }
                ],
                "type": "buffer"
            }
        ],
        "ver": "0.1"
    }
}