{
    "Shader": {
        "info": {
            "date": "1532967924",
            "description": "Visualization of various activation functions: LeRU, tanh, sigmoid, ELU, atan, softplus\nThe receptive field varies with different activation functions. ",
            "flags": 0,
            "hasliked": 0,
            "id": "4lccRB",
            "likes": 17,
            "name": "Visualization of Activation Func",
            "published": 3,
            "tags": [
                "field",
                "function",
                "sigmoid",
                "transfer",
                "tanh",
                "receptive",
                "activation",
                "leru",
                "softplus"
            ],
            "usePreview": 0,
            "username": "starea",
            "viewed": 813
        },
        "renderpass": [
            {
                "code": "/** \n * Visualization of various activation functions by Ruofei Du (DuRuofei.com)\n * ReLU, tanh, sigmoid, ELU, atan, softplus\n * The receptive field varies with different activation functions. \n *\n * RF_{l+1} = RF_i + (kernel_size_{l+1} - 1) \\times feature_stride_i\n * e.g., 1, (1+(3-1)*1)=3, 3+(3-1)*1 = 5\n * \n * The effect receptive field is smaller, and we should enrich receptive fields and discretize anchors over layers\n * \n * Reference: \n * [1] Understanding the Effective Receptive Field in Deep Convolutional Neural Networks. https://arxiv.org/abs/1701.04128\n * [2] Activation Functions: Neural Networks. https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6\n * [3] Kohonen's Self Organizing Feature Maps. http://www.ai-junkie.com/ann/som/som1.html\n *\n * Link to ShaderToy demo: https://www.shadertoy.com/view/4lccRB\n **/\n\n// The Sigmoid Function curve looks like a S-shape, a.k.a., Logistic Activation Function\n// The main reason why we use sigmoid function is because it exists between (0 to 1). \n// Therefore, it is especially used for models where we have to predict the probability as an output.\n// Since probability of anything exists only between the range of 0 and 1, sigmoid is the right choice.\n// The function is differentiable.That means, we can find the slope of the sigmoid curve at any two points.\n// The function is monotonic but functionâ€™s derivative is not.\n// The logistic sigmoid function can cause a neural network to get stuck at the training time.\n// The softmax function is a more generalized logistic activation function which is used for multiclass classification.\nfloat sigmoid(float a, float f) {\n\treturn 1.0 / (1.0 + exp(-f * a));\n}\n\n// The ReLU is the most used activation function in the world right now.\n// Rectified Linear Unit\n// Since, it is used in almost all the convolutional neural networks or deep learning.\n// The function and its derivative both are monotonic.\n// But the issue is that all the negative values become zero immediately which decreases \n// the ability of the model to fit or train from the data properly.\n// That means any negative input given to the ReLU activation function turns the value into zero immediately in the graph, \n// which in turns affects the resulting graph by not mapping the negative values appropriately.\n\nfloat ReLU(float x) {\n    return max(0.0, x);\n}\n\n\n// Leaky ReLU, a.k.a., Parameteric Rectified Linear Unit\n// The leak helps to increase the range of the ReLU function. Usually, the value of a is 0.01 or so.\n// When a is not 0.01 then it is called Randomized ReLU.\n// Therefore the range of the Leaky ReLU is (-infinity to infinity).\n// Both Leaky and Randomized ReLU functions are monotonic in nature. Also, their derivatives also monotonic in nature.\nfloat leakyReLU(float x) {\n    const float a = 0.01;\n    return step(x, 0.0) * a * x + step(0.0, x) * x; \n}\n\n// Exponential Linear Unit (ELU)\nfloat ELU(float x) {\n    const float a = 0.01;\n    return step(x, 0.0) * a * (exp(x) - 1.0) + step(0.0, x) * x;\n}\n\n// The softmax function is a more generalized logistic activation function which is used for multiclass classification.\nfloat softplus(float x) {\n    return log(1.0 + exp(x));\n}\n\nfloat TanH(float x) {\n\treturn 2.0 / (1.0 + exp(-2.0 * x)) - 1.0;    \n}\n\nfloat CHAR(vec2 p, int C) {\n    if (p.x < 0. || p.x > 1. || p.y < 0.|| p.y > 1.) return 0.;\n    return textureGrad(iChannel0, p/16. + fract(vec2(C, 15-C/16) / 16.), dFdx(p/16.),dFdy(p/16.) ).r;\n}\n\nvoid mainImage( out vec4 fragColor, in vec2 fragCoord )\n{\n    float ap = max(iResolution.x, iResolution.y) / min(iResolution.x, iResolution.y);\n    vec2 p = fragCoord / iResolution.xy;\n    vec2 uv = (fragCoord + fragCoord - iResolution.xy) / iResolution.y;\n    \n    vec3 col;\n    \n    if (p.x > 2.0 / 3.0) {\n        uv.y += 0.5 * step(p.y, 0.5) - 0.5 * step(0.5, p.y);\n        uv.x = uv.x + 1.0 * ap - 5.0 / 6.0 * 2.0 * ap;\n        uv *= 3.0;\n        \n        if (p.y > 0.5) {\n            col = vec3(1.0-sigmoid(length(uv), 1.0));\n        } else {\n            col = vec3(1.0-softplus(length(uv)));\n        }\n    } \n    else if (p.x > 1.0 / 3.0) {\n        uv.y += 0.5 * step(p.y, 0.5) - 0.5 * step(0.5, p.y);\n        uv *= 3.0;\n        if (p.y > 0.5) {\n            // tanh is also like logistic sigmoid but better. \n            // The range of the tanh function is from (-1 to 1). tanh is also sigmoidal (s - shaped).\n            // The advantage is that the negative inputs will be mapped strongly negative and \n            // the zero inputs will be mapped near zero in the tanh graph.\n            // The function is differentiable. The function is monotonic while its derivative is not monotonic.\n            // The tanh function is mainly used classification between two classes.\n            // Both tanh and logistic sigmoid activation functions are used in feed-forward nets.\n            col = vec3(1.0-tanh(length(uv)));\n        } else {\n        \tcol = vec3(1.0-atan(length(uv)));\n        }\n    } else {\n        uv.y += 0.5 * step(p.y, 0.5) - 0.5 * step(0.5, p.y);\n        uv.x = uv.x + 1.0 * ap - 1.0 / 6.0 * 2.0 * ap;\n        uv *= 3.0;\n        if (p.y > 0.5) {\n            col = vec3(1.0-ReLU(length(uv)));\n        } else {\n            col = vec3(1.0-ELU(length(uv)));\n        }\n    }\n\t\n    vec3 fontColor = vec3(1.0); \n    p.x *= ap;\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(0.,   7.5), 82));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(.5,   7.5), 69));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(1.,   7.5), 76));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(1.5,  7.5), 85));\n    \n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(9.,   7.5), 84));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(9.5,  7.5), 65));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(10.,  7.5), 78));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(10.5, 7.5), 72));\n    \n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(18.,  7.5), 83));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(18.5, 7.5), 73));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(19.,  7.5), 71));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(19.5, 7.5), 77));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(20.,  7.5), 79));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(20.5, 7.5), 73));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(21.,  7.5), 68));\n    \n    \n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(0.,   0.3), 69));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(0.5,  0.3), 76));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(1.0,  0.3), 85));\n    \n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(9.,   0.3), 65));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(9.5,  0.3), 84));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(10.,  0.3), 65));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(10.5, 0.3), 78));\n    \n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(18.,  0.3), 83));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(18.5, 0.3), 79));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(19.,  0.3), 70));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(19.5, 0.3), 84));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(20.,  0.3), 80));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(20.5, 0.3), 76));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(21.,  0.3), 85));\n    col = mix(col, fontColor, CHAR((p * 15.0) - vec2(21.5, 0.3), 83));\n\t\n    fragColor = vec4(col,1.0);\n}",
                "description": "",
                "inputs": [
                    {
                        "channel": 0,
                        "ctype": "texture",
                        "id": 49,
                        "published": 1,
                        "sampler": {
                            "filter": "mipmap",
                            "internal": "byte",
                            "srgb": "false",
                            "vflip": "true",
                            "wrap": "repeat"
                        },
                        "src": "/media/a/08b42b43ae9d3c0605da11d0eac86618ea888e62cdd9518ee8b9097488b31560.png"
                    }
                ],
                "name": "Image",
                "outputs": [
                    {
                        "channel": 0,
                        "id": 37
                    }
                ],
                "type": "image"
            }
        ],
        "ver": "0.1"
    }
}